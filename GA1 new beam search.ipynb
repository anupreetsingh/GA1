{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38OClBkHaIX3"
   },
   "source": [
    "# Anupreet Singh's GA1 Implementation(Campus ID-Uk43298)\n",
    "Disclaimer: \n",
    "* The markdown sections that say Tech Note are notes for myself as the developer. They don't give description in regards to structure of the Jupyter Notebook.\n",
    "* Any mardown section that doesn't say Tech Note/TN is intended for the perusal of the reviewer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrlzUTXWMcLO"
   },
   "outputs": [],
   "source": [
    "## run these 2 commands in a terminal to activate a virtual environment to be used for the purpose of this .ipynb file\n",
    "# cd '/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation'\n",
    "# source GA1env/bin/activate\n",
    "# then close file and open again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF80sTUQMcLP"
   },
   "source": [
    "##### Tech Note(Most Basic things about Coding):\n",
    "Statement are the most basic intructions that the interpreter executes. For Example: Assignment(x=6), control flow statements(if, for, while, with), import statement(import torch), function definition(def fun).  \n",
    "\n",
    "Two types of Parameters in Function Definition:-\n",
    "1. Requried Parameters are the Uninstantiated variables x and y in def fun(x, y, age=\"20\", year=\"2021\"), required parameters are often placed first in function definitions\n",
    "2. Optional Parameters are the instantiated variables age and year in def fun(x, y, age=\"20\", year=\"2021\"), these have a default value already in the function definition so it isnt necessary to pass arguments for them. They usually come after required parameters\n",
    "\n",
    "Two possible ways of passing arguments in a function:-\n",
    "1. You could either just send in argument as raw values or some variable name in the order that the parameters are set in the function definition.\n",
    "2. You could use the keyword argument irrespective of the order, where the name of parameter from the function definition is used to instantiate it to a value of choice in the function call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1-Data Preparation and Fine Tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9f4yIYIMcLP"
   },
   "source": [
    "##### TN(Pip Package):\n",
    "When I install a certain pip package it remains installed in my virtual environment directory even if I close my vs code entirely, but when I open the .ipynb file and select the kernel that points to the python interpreter in that virtual environment then I can import those package in the current runtime of my kernel by using import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59691,
     "status": "ok",
     "timestamp": 1733475623516,
     "user": {
      "displayName": "Anupreet Singh Sidhu",
      "userId": "10595161356432160963"
     },
     "user_tz": 300
    },
    "id": "MvoXj1rzOEB2",
    "outputId": "dbcdf5b6-61bc-4ae6-bb1c-afcc22bfd82f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.26.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.8.30)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, colorama, sacrebleu\n",
      "Successfully installed colorama-0.4.6 portalocker-3.0.0 sacrebleu-2.4.3\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.5.6)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.6.10)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (75.1.0)\n",
      "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets)\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.4)\n",
      "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
      "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.7.2)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
      "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.4)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.3.6)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.22.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
      "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.2)\n",
      "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jedi\n",
      "Successfully installed jedi-0.19.2\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
     ]
    }
   ],
   "source": [
    "## Installing Required Libraries\n",
    "%pip install transformers\n",
    "%pip install \"numpy<2\" #this installs numpy version 1. and is crucial as numpy version 2.0(which is automatically installed when installing transformers) does not work well with torch\n",
    "%pip install datasets\n",
    "%pip install torch\n",
    "%pip install 'accelerate>=0.26.0' #to be used for Trainer\n",
    "%pip install sacrebleu\n",
    "\n",
    "#These are needed only in jupyter notebook running locally to display things like progress bars in output cells\n",
    "%pip install ipywidgets\n",
    "%pip install sentencepiece\n",
    "%pip install tqdm #for showing progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 32530,
     "status": "ok",
     "timestamp": 1733475656043,
     "user": {
      "displayName": "Anupreet Singh Sidhu",
      "userId": "10595161356432160963"
     },
     "user_tz": 300
    },
    "id": "dulPlTvtKten"
   },
   "outputs": [],
   "source": [
    "## Importing Libraries\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer, MarianMTModel, MarianTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F #is a module that provides several functions for pytorch tensors commonly used in neural networks related tasks\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import os #to make changes to files in local disk\n",
    "import accelerate\n",
    "from tqdm import tqdm\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348,
     "referenced_widgets": [
      "adbb547788ca487490713d32bc4250e6",
      "3b735b3fcc9143a0a2d71d6a2e080917",
      "1250a925e7d54075bb77fe786c4a46b5",
      "84ead4ae903c460cb6808095d27b0de6",
      "7115f8d4a830471187da3471582ec9b3",
      "be3460aeba1345f3880666dcd00b5f73",
      "14484fa305a549a89eaaa2cbdbfaf5ce",
      "fadd31106d0040f8bbaeb2cea0aa63c3",
      "909736211af4453dbd4d8182dcc43369",
      "ed9e8b12d97442589bce6db4e6f5ce67",
      "3bc2db4f4a6842feb84120eca9cdc203",
      "b14dfb3ac719409bb9cc33737a5b7029",
      "dcfaf22176aa451eb7f777f5d2649cd5",
      "2e8cf6651c734b30bdaf116471104bf4",
      "9ff1f2f3ba1246bc969014ed0f700e2f",
      "cdfbeca7348649fcace275726dd0b4f0",
      "fe89d2cb1bf64b05b6bf8cb9790bd329",
      "18b22262fc0743ea8831b233d277c572",
      "7aa822851fd34ff28ac6c04d4fd0f6d9",
      "086f908260d84d97bb4d3d8912c0a2b8",
      "6407f72cf934474caedcc19f901fc00e",
      "9c7e8ce8482c414b90067a2e4804b848",
      "e20f5a4e42aa4854bbbe3d8e48b329f6",
      "d01c2b6c6c8445b8bd60d924f5eb3b07",
      "9e79a9b62a4448f89699a2882d23b6da",
      "9d98be13cfc1449e8234f62c3b9b7e19",
      "0b8936b90e514585b72f13c4817d9972",
      "7ce6a60ab6014cbeaa82a7bd48923664",
      "50251a019f154bcf9cdc7febefb2cbda",
      "5226f63a4e624a17864a9d721b54fcda",
      "4c091b74ebbd405bab8d85ee5cd4fe30",
      "09ee4dd3cef2463d8b737cb744cf08dd",
      "869999a5c12448a08e5b145d1c9dc235",
      "188de32fec5a4d99bf2c273f7f499caf",
      "d4b82c2e33f545209328ad9844431036",
      "e73f35d00fa3489c9d8e1c769cd23daa",
      "633b2b2083ca4aeebb414db8d9ccd73c",
      "75a767165bde458f9857392bc42b0e06",
      "5b81decca076416b87157df71421c6ba",
      "cdbbf4cb571a4c88a71fd75bdb6f1879",
      "8c21bc1c2f654fb5ab643358996c3f2e",
      "64d49d183a5a47ab9e8fbe7dc8b054c7",
      "88a5219ea8b748188857435e53937ff1",
      "35dbdbb9897e4f3caf4e20dfe634b68b",
      "cc2a4cf1aa40493abfee8dd75bd3c5a4",
      "0d7e39bb4bb84260816bb88c50f0c5b2",
      "5f52ab484d4745a9908cbd90a9dadd43",
      "5616732afb1041ec950140169a1f23a3",
      "d175477ce2914c509b199e7aa7303539",
      "243c9cff3283467da77d53498098f44f",
      "0be2e316cc7d4c318b7a297ab2b4487d",
      "52d26450f7c7480180e1fe2e25851950",
      "2470471eb85745fe8e840cc600909a22",
      "5d54ab52b9e84d3f83f7ddd5162ac1d2",
      "39b8714087e946deabd07f5febf9b345",
      "0ad89b826f9f497ead30f7baf8a22645",
      "e3153271b46b46b18802558d77e35e03",
      "569b65bef3714fe7b509a1a5009b4d23",
      "6fcad94f7ef24cc4928b9947c3fbf6a6",
      "53ca98b67d23472c94e69f5c0eced4bc",
      "ccb4bc48223849bfa9b3b7d175430889",
      "473a902fc7ff4d5f9c258c1c134e6b7c",
      "1f06d89519d54f4da8f77c8c5918ea5b",
      "170d50285d494223975f8acf8ae5f859",
      "ea42373920a04ea2bc0e27fb3326355d",
      "06cae619c2d54d20b7c7a96d7251f213",
      "82a6ab736045435a98b5d199a6cc635e",
      "b07f233f49034cc8b44af809a820f821",
      "7533d927e6d946db9f0514f158556457",
      "d41a83d52c5c4b2c8e596c10dbe9fd51",
      "7c14fae85ab7474f877603dfe05838db",
      "9b00f47f4fe64296b2fd0ca71cca76b7",
      "c0d0d6f8a901488ba897f87477fef023",
      "ecea6e3b42e24ce3a206069db4f667b5",
      "9c27544f090a4326933304d933ffcef4",
      "fa03bebf1d5740e3ba2e66cedebd642b",
      "476c5808e47447cb802ed84a29ba5713"
     ]
    },
    "executionInfo": {
     "elapsed": 17009,
     "status": "ok",
     "timestamp": 1733475673049,
     "user": {
      "displayName": "Anupreet Singh Sidhu",
      "userId": "10595161356432160963"
     },
     "user_tz": 300
    },
    "id": "EZKZ9XXqK5Yv",
    "outputId": "3890d6a6-1a40-4bdf-80d8-7c7a5225a424"
   },
   "outputs": [],
   "source": [
    "## Downloading the datasets\n",
    "\n",
    "#Uncomment one of the below to download wmt-16 or 19\n",
    "dataset = load_dataset('wmt16', 'ru-en') #This downloads the smaller dataset into our working environment\n",
    "#dataset.save_to_disk('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-16(en-ru)')\n",
    "#TN- .save_to_disk huggingface function already included in the dataset object(created uing load_dataset function) that saves processed dataset to our local storage, it always saves in .arrow files.\n",
    "\n",
    "#Uncomment one of the below to download wmt-19\n",
    "#dataset = load_dataset('wmt19', 'ru-en') #This downloads the bigger dataset into our working environment\n",
    "#dataset.save_to_disk('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-19(en-ru)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONZBkmBnMcLS"
   },
   "outputs": [],
   "source": [
    "## Loading dataset from local storage to our kernel\n",
    "dataset=load_from_disk('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-16(en-ru)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9KbbD9FMcLS"
   },
   "source": [
    "##### TN(Dataset Structure):\n",
    "1. For hugginggace datasets When you print dataset[\"train\"] what you see is not the actual arrangement of the train split of dataset but a high level summary of the datasets stucture, since the actual dataset is too big and it would be asinine to print it anyway.\n",
    "\n",
    "2. What is happening in the code in the second cell below is that you go to train split of the dataset, it has a column(the only feature as seen in the metadata of stucture). In that column you select which row you want to go to, each row has a dictionary(in this case key value pair of the 2 languages), and then you access the english part of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8piaRaqMcLS",
    "outputId": "0742114f-d83d-481c-879b-5080bed73a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 2818\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Seeing the structure of the dataset\n",
    "print(dataset[\"validation\"])\n",
    "\n",
    "for i in range(5):\n",
    "    print(dataset[\"train\"][\"translation\"][i][\"en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B512Ni6CMcLT"
   },
   "source": [
    "##### TN(Translation Using mT5):\n",
    "The two methods to generate an english to russian translation using mT5 are:\n",
    "1. Use a prefix in the english sentence indicating that task(which turns out performs extremely poorly so the second method is the basically the only option to get meaningful results)\n",
    "2. Fine tune the model on a large dataset containing pair of english and russian sequences so that it implicitly learns what to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LsXNovUMcLT",
    "outputId": "83f2320e-1714-4082-8bfd-98ddb8b0cd08"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "## Loading the mT5 tokenizer and model\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-base\")#this downloads the tokenizer into our local working environment\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oU9kbw9McLU",
    "outputId": "06642b29-895c-4eca-f706-d9b7a72187dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(62518, 512, padding_idx=62517)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(62518, 512, padding_idx=62517)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(62518, 512, padding_idx=62517)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=62518, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## moving the model to the correct device\n",
    "device=torch.device('mps') #moving the model to integrated GPU on macOS, used \"cuda\" for dedicated GPU and \"cpu\" for cpu\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfTs8JRqMcLU"
   },
   "source": [
    "##### TN (Tokenizer)\n",
    "In the Arguments of Tokenizer:\n",
    "* \"max_length\"=128 and \"truncation\"=True means that any sequence in the batch(of size 1000 by default) longer than will be truncated to 128\n",
    "* \"padding\"=True means that all sequences will be padded to the lenght of the longest sequence in the batch.\n",
    "* \"padding\"=max_length means that all sequences in the batch will be padded to the length of 128 in this case.\n",
    "\n",
    "\n",
    "In the Output of the tokenizer we get a dictionary containing two key-value pairs:\n",
    "* \"input_ids\" are the numerical representation of tokens in form of pytorch tensors. Pytorch tensors could be visualized as 2D matrix where each row correspond to token ID of one sentence and each column correspond to token ID in the sentence\n",
    "* \"attention mask\" tells which tokens are paded tokens so they can be ignored during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397,
     "referenced_widgets": [
      "4463c9290f15422a8adafe1cb295504b",
      "da60fabd31f54e32a22a5c8ba8984ae9",
      "028d3abeca024d53ae3e5cdae8a2bbd6",
      "810143c13796416798a7721f85ef630d",
      "a374c8b87b134ad7b27fec4acab742de",
      "7ebbcb301aff41e7bd9f273c4347f085",
      "fe57e399e0714707aa48c5b36b2dbc87",
      "33cb2b8562cc401093bf28905f9c7992",
      "7be52ff9086e4a23aea03f9ee8c9a5eb",
      "d11a0d13b20c40ada5b4e027a5f89b1b",
      "dbf2a33450264864908701bbe9c3f7e3",
      "7d8bf81c515d45019ac33e8cac4cfa47",
      "cb4b2ea6bc884d63af9d6b0fd091d1e5",
      "b2c8afc3a8954e659329e01d48e05cc0",
      "119a5b5e698642809a45483e2f83caa3"
     ]
    },
    "id": "vB6w4SCgMKjg",
    "outputId": "06a7cf74-09b5-4a85-e9a8-8629cf3f152c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8bf81c515d45019ac33e8cac4cfa47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1516162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4b2ea6bc884d63af9d6b0fd091d1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/6 shards):   0%|          | 0/1516162 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c8afc3a8954e659329e01d48e05cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119a5b5e698642809a45483e2f83caa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Preprocessing/tokenizing the WMT dataset to generate vocabulary\n",
    "def preprocess_function(examples):\n",
    "    source_tokens = tokenizer([example['en'] for example in examples['translation']],\n",
    "                        return_tensors='pt',\n",
    "                        padding=\"max_length\", #this is done because the Trainer() expects all the sequence in its training batch to be of the same length, so we make all sequence of the same lenght since training batch cant be made as big as 1000 due to computation constraints and accuracy concerns.\n",
    "                        truncation=True,\n",
    "                        max_length=128)# altough in the train split the longest english and russian sequences are 5194 and 4886 words long, but sinces we just need to compare two performance for now we will use max_length=128, we could increase this to 256 or 512 but that would just require extra memory\n",
    "    target_tokens = tokenizer([example['ru'] for example in examples['translation']],\n",
    "                        return_tensors='pt',\n",
    "                        padding=\"max_length\",\n",
    "                        truncation=True,\n",
    "                        max_length=128)\n",
    "    source_tokens['labels'] = target_tokens['input_ids']#Here we create a key-labels and store input_ids of target language as value in it to correspond with input_ids of source language,which will be used for model training\n",
    "    # source_tokens['labels_attention_mask'] = target_tokens['attention_mask']#we could also include target attention masks if needed\n",
    "\n",
    "    return source_tokens#and then we just return the source_tokens as it has both language input_ids too\n",
    "\n",
    "\n",
    "# Process and save immediately\n",
    "#TN- The .map() function processes the dataset in batches of size 1000 by default. During this, intermediate results (like tokenized data) are cached in our local directory to avoid redoing the computations in the future.\n",
    "train_T_dataset = dataset['train'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n",
    "# Removing the 'translation' column from the tokenized dataset and then saving it to local disk\n",
    "train_T_dataset = train_T_dataset.remove_columns(['translation'])\n",
    "train_T_dataset.save_to_disk('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-16(en-ru) train_T')\n",
    "\n",
    "\n",
    "eval_T_dataset = dataset['validation'].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")\n",
    "# Removing the 'translation' column from the tokenized dataset and then saving it to local disk\n",
    "train_T_dataset = eval_T_dataset.remove_columns(['translation'])\n",
    "eval_T_dataset.save_to_disk('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-16(en-ru) eval_T')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#You could use the line below as an argument in .map to remove the column from the dataset loaded in kernel to free up some memory, but it is not gonna do much here\n",
    "#remove_columns=dataset['train'].column_names\n",
    "#remove_columns removes the provided column name(in this case just one column named translation,which is basically the entire dataset) from kernel memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvAg4oV7McLV",
    "outputId": "8bc53b95-0799-45b4-feca-3fa147c164a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting cache file: /Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-16(en-ru)/train/cache-7af736c62e1f0b78.arrow\n",
      "Deleting cache file: /Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-16(en-ru)/validation/cache-5122a4982e8ec6c1.arrow\n"
     ]
    }
   ],
   "source": [
    "## Deleting the cache on the local diskc created during preprocessing\n",
    "#Since we saved the processed dataset to disk we can now delete the cache files which have been created in our original dataset folder when it was being tokenized by the .map function\n",
    "# Define the path to your dataset directory\n",
    "train_dir = '/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-16(en-ru)/train'\n",
    "eval_dir = '/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-16(en-ru)/validation'\n",
    "# Function to delete cache files\n",
    "def delete_cache_files(directory):\n",
    "    # Loop through files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Check if the file contains \"cache\" in its name and ends with .arrow\n",
    "        if 'cache' in filename and filename.endswith(\".arrow\"):\n",
    "            print(f\"Deleting cache file: {file_path}\")\n",
    "            os.remove(file_path)  # Delete the file\n",
    "\n",
    "# Call the function to delete cache after saving\n",
    "delete_cache_files(train_dir)\n",
    "delete_cache_files(eval_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23xze-9EMcLV"
   },
   "outputs": [],
   "source": [
    "##loading the Tokenized dataset from local disk\n",
    "train_T_dataset=load_from_disk('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-16(en-ru) train_T')\n",
    "eval_T_dataset=load_from_disk('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/WMT-16(en-ru) eval_T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNPs3OfXMcLV"
   },
   "source": [
    "##### TN(Tokenized dataset):\n",
    "1. Each feature of the dataset is actually a column, and the tokenized dataset here contains the original translation column from the train split as well as 4 other columns that we put into it using the tokenizer.\n",
    "2. We removed the \"translation\" column and dont add the \"labels_attention_mask\" column for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1-O4jUEMcLV"
   },
   "source": [
    "##### TN (Trainer):\n",
    "1. The batch size tells that in 1 step how many examples does the optimizer go over before updating the models parameters\n",
    "\n",
    "2. By default the Trainer class uses the AdamW optimizer for adjusting the weights of the transformer models like mT5. AdamW optimizer can be thought of as an optimization algorithm like gradient descent but a much better version of it.\n",
    "\n",
    "3. weight_decay works by penalizing larger weights to avoid overfitting to the training set. This is because if the model has learned very large weights for a particular feature then the predictions may be dominated by one or more such features, causing the model to overfit to the peculiarities of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "703c9fc163c0456e9411c067a15f9869"
     ]
    },
    "id": "0dJendigMcLW",
    "outputId": "3ef82766-32eb-40d5-cda5-8e2792996101"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/GA1env/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/GA1env/lib/python3.12/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/var/folders/mg/wpj49_8s7yb8s_vtb6b2x7xc0000gn/T/ipykernel_28147/2279088603.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703c9fc163c0456e9411c067a15f9869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454851 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 31\u001b[0m\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                       \u001b[38;5;66;03m# the model to train\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                \u001b[38;5;66;03m# training arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# first_100_rows = train_T_dataset.select(range(100)) , select()could be used to select the required rows from the dataset\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Start the fine-tuning process\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/Model/Fine-tuned Model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/GA1env/lib/python3.12/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/GA1env/lib/python3.12/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/GA1env/lib/python3.12/site-packages/transformers/trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/GA1env/lib/python3.12/site-packages/accelerate/accelerator.py:2241\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/GA1env/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/GA1env/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Fine tuning the model to the dataset\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/Model/model training checkpoints', # output directory where the model predictions and checkpoints will be saved, checkpoints are state of the model at that time\n",
    "    save_steps=10_000,               # save checkpoints after every 10,000 steps in the output_dir\n",
    "    save_total_limit=2,              # limit the number of checkpoints to save\n",
    "    logging_dir='/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/Model/Model training log', # directory for storing logs\n",
    "    logging_steps=500,               # log training metrics(like loss) at every 500 steps in the logging_dir\n",
    "    evaluation_strategy=\"epoch\",     # by this strategy we evaluate the model after every epoch\n",
    "    learning_rate=5e-5,              # learning rate for the optimizer\n",
    "    per_device_train_batch_size=10,   # batch size for training\n",
    "    per_device_eval_batch_size=4,    # batch size for evaluation\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    no_cuda=True,                    #This automatically sets model.device=cpu, I am Using the CPU for tuning the model, since my GPU is not good\n",
    "    dataloader_num_workers=4,        # Use 4 CPU cores for data loading\n",
    ")\n",
    "\n",
    "# Initialize the Trainer, the Trainer class from huggingface handles fine tuning the model(AKA adjusting the models parameters according to the dataset to minimize loss)\n",
    "trainer = Trainer(\n",
    "    model=model,                       # the model to train\n",
    "    args=training_args,                # training arguments\n",
    "    train_dataset=train_T_dataset,   # the tokenized dataset for training\n",
    "    eval_dataset=eval_T_dataset.select(range(10)),    # the tokenized dataset for evaluation (same dataset for simplicity)\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# first_100_rows = train_T_dataset.select(range(100)) , select()could be used to select the required rows from the dataset\n",
    "\n",
    "\n",
    "# Start the fine-tuning process\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/Model/Fine-tuned Model')\n",
    "tokenizer.save_pretrained('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/Model/Fine tuned Tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UUaH4n2McLW"
   },
   "outputs": [],
   "source": [
    "## Loading the fine tuned model\n",
    "# Load the model from local disk\n",
    "model = MT5ForConditionalGeneration.from_pretrained('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/Model/Fine-tuned Model')\n",
    "\n",
    "# Load the tokenizer from local disk\n",
    "tokenizer = MT5Tokenizer.from_pretrained('/Users/manpreetsingh/Downloads/UMBC/NLP 673/Graduate Assesment/GA1 Implementation/Datasets/Model/Fine tuned Tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Using the Model's .generate function to translate the eval split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbflDZpOMcLW"
   },
   "source": [
    "##### TN(Translating using .generate):\n",
    "1. Putting the model in evaluation mode is necessary because it then dropout is disabled, so no neurons are randomly dropped and the whole network is used for making predicitions.\n",
    "\n",
    "2. The models .generate function expects all tokenized sequences to be of the same length, which is why attention mask is used to tell the model which token ids are padded in a sequence so it could ignore it. This is why we pad all sequneces to length of longest sentence in the eval dataset by padding=True.\n",
    "\n",
    "3. When calculating BLEU score using sacrebleu.corpus(hypothesis, references).The function expects first argument \"hypothesis\" to be a list of string sequences generated by the model and second argument \"reference\" to be list(Outer) of lists(Inner) of reference translations where each list(inner) is one set of reference string sequences.This is so that each hypothesis sequences could possibly be compared to more than 1 possible reference translation.\n",
    "\n",
    "4. Another Big Thing I found about BLEU score(through falling in a valley of errors) is that it does not raise an error even if their is signicant mismatch between the number of hypotheses and reference sequences so always check length of the list containing sequences before calculating BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277,
     "referenced_widgets": [
      "129805d8e3824296a5b7d71d2908f418",
      "41dbbe791ff7441ba81ada90bd2c6d92",
      "ac30a9e2da75411cb4fc67833d670eff",
      "3c3fa86c347845eabcabee90f59120fd",
      "1858f92c7f054ad79116ecb5651e05b9",
      "f4cff771ef77407fa1a18e5548467457",
      "0b4a8baf204d4ce0b0b37cee816fdbc6",
      "d06807057e444c75903bb2025340e8c4",
      "79bb9344bc424697b18e36429b6eec4a",
      "903c56e5e4624049a5a53d1ab693e004",
      "2aa35e39c3334c1cad4da515e10a7ff0",
      "93a053b30eb74061b467c239710ea422",
      "da0c64dba28a428a9b345f21ec81883c",
      "556cecdea2c84412a8e364b175c41acf",
      "97f0875d41e545b8b500ce6ba7b81471",
      "d8da40f672314e669b6c83c6ec6d4ce4",
      "1f1f903e12ce4ee994a975f5fd575740",
      "aa926e098e0d41278247cd3635b0363d",
      "2cc09841c13b404fbc633dc73362fec3",
      "bb02a36931bd47b19efac803b3b5d371",
      "90c5cb554daf49eea0744ce1fe2a8b33",
      "4706c3a0eaad418fab93513aee76c2e5",
      "c9a5ef7c8a9940af8110c3fec3834870",
      "20bd7ecbc7214a1cba1070782e3a2730",
      "6c357670a18845b2bcf43c50f1dfd217",
      "bebe60db38544e2ab7fc92b63cb800fb",
      "2eabbd91292f4edf8d8c9f0d28a63fed",
      "5ab866bb475446cfb3b05418c3f647c3",
      "f1ac1ecf6a9045358d732f0d6b1f9b79",
      "4284fdefcf994b86a9074d3484a17204",
      "8757719e8a614c70873ca4e97ff61fc2",
      "1aaed2a7462842a68ee4c51ae31ca0fd",
      "8dc72a507e914a4ea6e255ee91da63ee",
      "7402994cb51b463fae900c5c77bd01d0",
      "91ddefbcaf74464395e062620158528f",
      "cd22b855862c4622bc98c25c12b75ce2",
      "10867f5fbda745b998402670846c745b",
      "85371c7a17274c10b007488f428cdf1f",
      "c95399dcaa774f57b27febab43cecc44",
      "716affa78b7f4b4fadf118189e54601c",
      "b7d075e89ab941299a0776c528a40591",
      "2b3dcf5dcfe54414ae565e1b949c77ae",
      "bf09a4cfcf5e4f399283f027e0873638",
      "b34fdb1c9f3441718fd2a5f020a13304",
      "3933e32825584d60a2869d1e43fedc5b",
      "758ac985798a4787a7a71152c9720257",
      "eb8225580b7b4e7fbe0e6ace9cba81c8",
      "e2e2ad09a5344601ac6d9d5b32d120e7",
      "6316e90c6b99422a88d91d1dd4b7773f",
      "54808fb2dbf342468c4366a24fe1cac1",
      "be9ce2b517a14583b92b4ada34332dd4",
      "ba8d2018a1c1448f8c60896089a5a48f",
      "249ccc99b69248028204cd8d4246553b",
      "54bebf90a0d94ba5af048c1c48244e4f",
      "e6dd792e3a2a4ef9b8b612687d756820",
      "0d9eaa8a86b14fb1908655bb54ebd653",
      "71e509449c0d4c6cb22beb7b4d188f64",
      "86f763b88c1346ddabe2c220ef97378e",
      "b6b69c495c5a48189588bd6c4603ef59",
      "8d195114a8924734a0b65aee2eda03fc",
      "ef7608c62aef432c988ce68ef9ec6ef9",
      "f053944ad1eb4145b82e7e9c7fd793a0",
      "c1c876415334463a8d939e13b0651d7f",
      "6c4e66d24ed949609a0b5edee4424e95",
      "da465b6209e7426690675a84c0ca6a6f",
      "6c756938690e486fb39b4c02280ed98e",
      "c3a50cec758f4029a1533960ef613b55",
      "b4d81ed8a52545aa87afc931caffd30e",
      "0fd29c5d9dce47f099165770aa9aadbb",
      "5aae6a9322e34c1083a69f5b4215a3f0",
      "184426f440be45b8a12e70aaf1ee4fe5",
      "4ac18157909843e7988bd16a7e25d841",
      "ce386c557f884dd8926d8c5e498ed7a2",
      "4523290cd5164a888da737020c61816a",
      "3d20b18d095840abadf655019c736fa4",
      "d83611947a67419a8254c64f980cf740",
      "cdb659084c684fd0abbece743e7c9997"
     ]
    },
    "executionInfo": {
     "elapsed": 9106,
     "status": "ok",
     "timestamp": 1733475682139,
     "user": {
      "displayName": "Anupreet Singh Sidhu",
      "userId": "10595161356432160963"
     },
     "user_tz": 300
    },
    "id": "FV2TFk19McLW",
    "outputId": "aa67d363-8fb4-4f3e-8b2c-091e16b5d96f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "129805d8e3824296a5b7d71d2908f418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a053b30eb74061b467c239710ea422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/803k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a5ef7c8a9940af8110c3fec3834870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7402994cb51b463fae900c5c77bd01d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3933e32825584d60a2869d1e43fedc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9eaa8a86b14fb1908655bb54ebd653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/307M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a50cec758f4029a1533960ef613b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Loading Helinski Tokenizer and Model\n",
    "# Loading the tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")#this downloads the tokenizer into our local working environment\n",
    "\n",
    "# Loading the pre-trained model for conditional generation (translation)\n",
    "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1733475682859,
     "user": {
      "displayName": "Anupreet Singh Sidhu",
      "userId": "10595161356432160963"
     },
     "user_tz": 300
    },
    "id": "_3Fq4BGTMcLW",
    "outputId": "32949a84-6af3-47cd-ace1-ebd895e18985"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(62518, 512, padding_idx=62517)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(62518, 512, padding_idx=62517)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(62518, 512, padding_idx=62517)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=62518, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## moving the model to the correct device\n",
    "device=torch.device('cuda') #moving the model to integrated GPU on macOS, used \"cuda\" for dedicated GPU and \"cpu\" for cpu\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKwavo48McLX",
    "outputId": "224b923a-446d-405f-a9d0-af790c085930"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94/94 [40:05<00:00, 25.60s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for validation split using .generate:BLEU = 27.11 56.9/32.9/21.0/13.8 (BP = 1.000 ratio = 1.027 hyp_len = 57419 ref_len = 55920)\n"
     ]
    }
   ],
   "source": [
    "## Using .generate to get translation of \"validation\" split of dataset\n",
    "eval_dataset=dataset[\"validation\"]\n",
    "generation_batch_size=30 #increase or decrease as per memory available in models processing unit\n",
    "model.eval()# Putting the model in eval mode\n",
    "def translate(dataset):\n",
    "    #tokenize the dataset at once because it is a small dataset and tokenization does not need to be done in batches\n",
    "    dataset_t = tokenizer([row[\"en\"] for row in dataset[\"translation\"]],\n",
    "                           return_tensors=\"pt\",\n",
    "                           padding=True,\n",
    "                           truncation=True,\n",
    "                           max_length=128).to(device) #we move the tensors to the device that our model is on\n",
    "\n",
    "    # Generate translation for batches of tokenized dataset and add them to translated_dataset list, we do this in batches to speed up the process by making use of the parallel procesing but we dont do it all at once because of possible memory constraints.\n",
    "    input_ids=dataset_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, i.e. lenght of list passed in tokenizer, sequence length)\n",
    "    attention_mask=dataset_t[\"attention_mask\"] #This will be 2D tensor of shape()\n",
    "    translated_dataset=[] #The list of generated translation which will be returned from the function\n",
    "    for i in tqdm(range(0,input_ids.shape[0], generation_batch_size)):\n",
    "        batch_input_ids= input_ids[i:i+generation_batch_size] #slicing for tensors works in the same way as slicing for lists, so if i+generation_batch_size>input_ids.shape[0] then slicing will just return a tensor until the last element of the parent tensor.\n",
    "        batch_attention_mask=attention_mask[i:i+generation_batch_size]\n",
    "        outputs = model.generate(input_ids=batch_input_ids, attention_mask=batch_attention_mask, num_beams=4, early_stopping=True)\n",
    "        translated_batch=tokenizer.batch_decode(outputs, skip_special_tokens=True) # tokenizer.decode could also handle 2D tensor containing muliple token_ids but .batch_decode is said to handle it faster\n",
    "        translated_dataset.extend(translated_batch) # .extend adds each element from an iterable to a list, whereas if we used .append here it would have added the entire iterable to the list as a single element\n",
    "\n",
    "    return translated_dataset\n",
    "\n",
    "\n",
    "# Translate all EN sentences\n",
    "model_generated_list = translate(eval_dataset)\n",
    "reference_list=[[row[\"ru\"] for row in eval_dataset[\"translation\"]]] #This creates a list containing one list of string sequences, a list containing one set of reference translations\n",
    "\n",
    "#BLEU score for model generation\n",
    "bleu_score = sacrebleu.corpus_bleu(model_generated_list, reference_list)\n",
    "print(f\"BLEU score for validation split using .generate:{bleu_score}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je_yNRaCMcLX"
   },
   "source": [
    "##### TN(Translating using custom function \"scratch_beam_search\"):\n",
    "1. As per the requirements of the task, we just replace .generate function from the cell above with our custom function that behaves exactly the same.\n",
    "2. By inspection of tokenizer.special_tokens_map and model.config for the helsinki model we find out that:\n",
    "    * eos_token_id=0\n",
    "    * unk_token_ids=1\n",
    "    * pad_token_id=62517\n",
    "    * decoder_start_token_id=62517, so the model is using the same token id to padd sequences and as a starting token\n",
    "3. For the tokenizer, the encoder converts text into token_ids and the decoder converts token_ids back into human readable text\n",
    "\n",
    "4. For the Model, the encoder process the input_ids of the source sequence and produces a hidden representation(context vector) that summarizes its meaning and the decoder takes encoder hidden representation and decoder_inputs_ids(output tokens generated upto that point) and produces logits corresponding to each next token for every sequence of decoder_input_ids. This process of generating logits once is called a Forward Pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Using the Model's forward pass in custom beam search function to translate the Eval split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1288789,
     "status": "ok",
     "timestamp": 1733477055524,
     "user": {
      "displayName": "Anupreet Singh Sidhu",
      "userId": "10595161356432160963"
     },
     "user_tz": 300
    },
    "id": "ikreuXlDMcLY",
    "outputId": "c3224c9e-fe50-4285-842a-9a55fb97e104"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2818/2818 [21:27<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for validation split using scratch_beam_search:BLEU = 27.48 57.4/33.3/21.3/14.0 (BP = 1.000 ratio = 1.012 hyp_len = 56568 ref_len = 55920)\n"
     ]
    }
   ],
   "source": [
    "## Using Custom function \"scratch_beam_search\" in place of .generate() to Translate Validation split of dataset\n",
    "eval_dataset=dataset[\"validation\"]\n",
    "reference_list_BLEU=[[row[\"ru\"] for row in eval_dataset[\"translation\"]]] #This creates a list containing one list of string sequences, a list containing one set of reference translations\n",
    "generation_batch_size=1 #increase or decrease as per memory available in models processing unit\n",
    "model.eval()# Putting the model in eval mode\n",
    "\n",
    "def scratch_beam_search(input_ids, attention_mask, num_beams=4, max_length=128, early_stopping=True):\n",
    "\n",
    "    original_batch_size = input_ids.shape[0] #This  original_batch_size will be the number of sequences sent into scratch_beam_search function\n",
    "    \"\"\"\n",
    "    This code below was causing the problem of not being able to use genertaion_batch_size>1 cause we were using .repeat function when we should have used repeat_interleave() function\n",
    "    The problem was because it would cause misalignment with decoder_input_ids when they are sent in the model during Forward pass\n",
    "\n",
    "    # Create a copy of input_ids and attention masks of each sequence for each beam\n",
    "    input_ids = input_ids.repeat(num_beams, 1) #.repeat is a pytorch function that creates a tensor by repeating the entire tensor along a its dimensions the specified number of times(number of copies along dim=0(row), number of copies of along dim=1(column)), here it creates \"num_beams\" copies of input_ids along the 1st dimension(batch_size) of the 2D tensor, and keeps everything along 2nd dimension(sequence length) unchanged since we pass in the 1\n",
    "    attention_mask = attention_mask.repeat(num_beams, 1)  #since we are creating as much copies as num_beams you could see why more beams would result in more memory usage and computational overhead\n",
    "\n",
    "    \"\"\"\n",
    "    input_ids = input_ids.repeat_interleave(num_beams, dim=0) #.repeat_interleave() repeats each element of along num_beam number of time along the specified dimension\n",
    "    attention_mask = attention_mask.repeat_interleave(num_beams, dim=0) #since we are creating as much copies as num_beams you could see why more beams would result in more memory usage and computational overhead\n",
    "\n",
    "\n",
    "    # Creating a 2D-tensor with one \"beginning of sentence\" token_id for each possible output sequence(input sequences*number of beams)\n",
    "    decoder_input_ids = torch.full(\n",
    "                        (original_batch_size * num_beams, 1), #torch.full allows us to create a tensor of shape=(batch_size * num_beams, 1) and fill each of its elements with value=model.config.decoder_start_token_id(62517 in case of helsinki)\n",
    "                        model.config.decoder_start_token_id,\n",
    "                        dtype=torch.long, #an optional argument to specify the data type of each element, torch.long is suitable for storing 64 bit signed integers\n",
    "                        device=device # an optional argument to specify which device to store the tensor on, This is a fresh tensor so it best to store it on the device that our model is on\n",
    "                        )\n",
    "    #batch_size of inputs_ids, attention masks, and decoder_input_ids is orginal_batch_size time num_beams.\n",
    "\n",
    "    # Creating a 1D tensor for storing total scores for each sequence upto current step in decoder_input_ids\n",
    "    beam_scores = torch.zeros(              #torch.zeros creates a tensor with all elements set to zeros\n",
    "                    original_batch_size * num_beams, #since we only pass one value in shape, it creates a 1D tensor\n",
    "                    dtype=torch.float, #torch.float refers to torch.float32 used for storing 32 bit floating point numbers\n",
    "                    device=device\n",
    "                    )\n",
    "    #finished_beams = torch.zeros(original_batch_size * num_beams, dtype=torch.bool, device=device) #keeping track of which beams have generated EOS tokens and finished generation\n",
    "    Start=True ##initial Condition to be checked only once during the first forward pass\n",
    "    # Generate sequences\n",
    "    for step in range(max_length):\n",
    "\n",
    "        with torch.no_grad(): #with statement allows us to use context manager(CM) to ensure proper setup(_enter_ method of CM) and cleanup(_exit_ method of CM) and manage resources effectively. torch.no_grad in a CM that disables gradient computation since it is an unnecessary computation overhead during inference.\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids\n",
    "                ) #This will be a dictionary like object containing a lot of things like hidden states,etc, but we are most interested in logits\n",
    "            \"\"\"\n",
    "            Talking about some important Tensor Operations:\n",
    "\n",
    "            Broadcasting:\n",
    "            In pytorch, a process called broadcasting happens just before an element wise operation that requires two tensors with same number of dimensions to align in shape.\n",
    "            Example Adding tensor A of shape(4,6,8) with tensor B of shape(4,1,1) then the scaler values in tensor B along the dimensions with only 1 element are duplicated to match the shape of the tensor A.\n",
    "            But broadcasting only happens when two tensors have same size of some dimensions and some dimensions of size 1.\n",
    "\n",
    "            tensor.view(size of dim=0, size of dim=1.....) is used to create a reshaped tensor with the same number of elements as the original tensor.\n",
    "            It can add new dimensions of a specific size as long as the number of element in the original tensor could be accomodated.\n",
    "            tensor.view(-1,10) tell us to accomodate 10 elements in dim=10 and infer what number of element to accomodate in dim=0 such that the total number of elements remain the same.\n",
    "\n",
    "            tensor.unsqueeze() is used to add a singleton dimension(dimension of size 1) at a specified index\n",
    "            tensor.unsequeeze(1) adds a singleton dimension at index=1\n",
    "\n",
    "            tensor.squeeze() is used remove a singleton dimension from a specified index\n",
    "\n",
    "            tensor.expand() is used to basically perform broadcasting on command, you would send in sizes of dimension of tensor which are capable of broadcasting and get a result.\n",
    "            This means that the size you send in should either be the same as sizes of dimension of original tensor OR in case of singleton dimension in original tensor you could send in a greater number to stretch along that dimension\n",
    "            tensor.expand(-1, 4) tells to keep dim=0 as is and basically stretch dim=1 to size 4, this is conditioned on the fact that the dimension to be stretched is of size 1 otherwise an error will occur\n",
    "\n",
    "            tensor.expand_as(target_tensor) is used to perform broadcasting on tensor conditioned on the fact that the size of target_tensor is suitable for broadcasting\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            # Creating a 2D tensor Logits\n",
    "            logits = outputs.logits[:, -1, :] #output.logits is a 3-D tensor of shape(original_batch_size*num_beam, current sequence length in decoder_input_ids, vocab size). where each element is the logit of each token_id of each sequence for each element of the vocabulary\n",
    "                                              #By slicing we choose the last token_id of each sequence it to get a 2-D tensor of shape(original_batch_size*num_beam, vocab_size) where each element is a logit of last token_id of each sequence element for each element of the vocabulary\n",
    "            #print(f\"Logits at step {step}:\\n{logits}\")\n",
    "            # Using Boolean masking to mark finished beams (those that already generated EOS)\n",
    "            #finished_mask = finished_beams.view(-1, 1).expand(-1, logits.size(-1))  # finished_mask will be a 2D of shape(original_batch_size * num_beams, vocab_size) with values of True/False repeated in each row(representing each beam) upto the vocab_size\n",
    "            #logits[finished_mask] = -float('inf')  # Making logits of lasts token for all elements of vocabulary for the finished beams negative infinity, By sending in boolean mask tensor of the same shape as the logits tensor the operation is applied to all logits elements that correspond with True in finished mask.\n",
    "                                                    #Making Logit for all words in vocabulary negative infinity means probability=0 after softmax, and log of that is a big negative value, when that big negative value is combined with beam_score to get next_score it ensures that these beams will now not come in the topk.\n",
    "                                                    #This ensures that only active beams (those not yet finished) are considered during the top-k selection.\n",
    "\n",
    "            # Creating 2D tensor log probabilities\n",
    "            log_probs = F.log_softmax(logits, dim=-1) #performs softmax on all logits normalizing them to probability of 0-1, then takes log of that so the highest probability is turend into the largest number(smallest negative value).\n",
    "                                                      #dim=-1 takes softmax along the last dimension(2nd dimension with size=vocab_size in this case) which means that normalization happens for each row(last token for each sequence of decoder_input_ids)\n",
    "            #print(f\"Log_probs at step {step}:\\n{log_probs}\")\n",
    "            #get top num_beams scores from each beam\n",
    "            total_top_log_probs, total_top_token_ids=torch.topk(log_probs, num_beams, dim=1) #give 2 tensors of shape(original_batch_size*num_beams, num_beams) where each row contains scores top num_beam scores for that row and the corresponding token_ids in descending\n",
    "            #print(f\"Topk log_probs at step {step}:\\n{total_top_log_probs}\")\n",
    "            #print(f\"Topk token_ids at step {step}:\\n{total_top_token_ids}\")\n",
    "\n",
    "            if Start==True:\n",
    "                new_tokens=total_top_token_ids[::num_beams].reshape(-1,1) #using slicing with step value num_beam we take the top 4 tokens of a beam of a sequence and make a tensor containing new tokens for each beam, .reshape is used in place of .view() becauce we used slicing and so tensor is not contiguous in memory hence .view wont work\n",
    "                decoder_input_ids=torch.cat([decoder_input_ids[torch.arange(original_batch_size*num_beams)],new_tokens], dim=1) #adding first 4 tokens to different beam of each sequence\n",
    "                beam_scores=total_top_log_probs[::num_beams].reshape(-1) #slicing to make tensor of shape(original_batch_size,num_beams) and then we flatten it using .reshape(-1)\n",
    "                Start=False\n",
    "                #print(f\"Decoder_input_ids at step{step}:\\n{decoder_input_ids}\")\n",
    "                #print(f\"Beam score at step{step}:\\n{beam_scores}\")\n",
    "\n",
    "                continue\n",
    "\n",
    "            #calculate total scores for each beams top 4 tokens\n",
    "            total_top_score=total_top_log_probs+beam_scores.unsqueeze(1)\n",
    "            #print(f\"Total score of each topk token of each beam scores at step {step}:\\n{total_top_score}\")\n",
    "\n",
    "\n",
    "            #reshape that\n",
    "            reshaped_total_top_score=total_top_score.view(original_batch_size, num_beams*num_beams)\n",
    "\n",
    "            #Getting the best scores for this step in generation\n",
    "            new_top_scores,new_token_indices=torch.topk(reshaped_total_top_score,num_beams, dim=1) #gives 2 tensors of shape (original_batch_size, num_beam) containing the top num_beam score from all total_scores of one sequence in each row in descending order and their index in that row in total_top_score_reshaped\n",
    "            #print(f\"Total Top scores for new tokens of each sequence scores at step {step}:\\n{new_top_scores}\")\n",
    "            #calculating beam indices for the new tokens of each sequence with best score\n",
    "            beam_indices=new_token_indices//num_beams #elements of a row in this tensor will tell which token originally belonged to which beam for a sequence\n",
    "            #reshaping beam_indices and scaling indices of elements from row level to global level in the tensor\n",
    "            base_indices=torch.arange(original_batch_size,device=device).unsqueeze(1) * num_beams\n",
    "            scaled_beam_indices=beam_indices+base_indices #Tensor of shape shape (original_batch_size, num_beam)\n",
    "            flattened_scaled_beam_indices=scaled_beam_indices.view(-1) #making a 1D tensor of shape(original_batch_size*num_beam)\n",
    "\n",
    "            #By Advance Indexing using these scaled indices we will obtain sequences from decoder_input_ids of the respective beam that the indices point to\n",
    "            prev_decoder_inputs=decoder_input_ids[flattened_scaled_beam_indices]\n",
    "\n",
    "            #Getting the token_ids of the tokens to be added\n",
    "            reshaped_top_total_token_ids=total_top_token_ids.view(original_batch_size, num_beams*num_beams)\n",
    "            \"\"\"\n",
    "            Here each row will contain the token ids of the top scoring tokens of all beams of a sequence just like the structure of reshaped_total_top_score\n",
    "            Since new_token_indices tells the row index of the topk elements of that row, the same indices will also point to the token_ids in reshaped_top_total_token_ids\n",
    "            So we make a vector of shape (original_batch_size,1) where each element will help us point to a row indices in reshaped_top_total_token_ids\n",
    "            And then new_token_indices has column indices which help us point to columns of reshaped_top_total_token_ids for that row\n",
    "            in this case of Advacne indexing because we use two rows and columns we get a new tensor of shape of rows and columns used\n",
    "            \"\"\"\n",
    "\n",
    "            raw_new_tokens = reshaped_top_total_token_ids[torch.arange(original_batch_size,device=device).unsqueeze(1), new_token_indices] #the token_ids for the new tokens in a Tensor of shape(original_batch_size, num_beams)\n",
    "            new_tokens = raw_new_tokens.view(original_batch_size*num_beams,1) #2D Tensor of shape(original_batch_size*num_beam,1) made to be suited for concatenation to decoder_input_ids\n",
    "            decoder_input_ids=torch.cat([prev_decoder_inputs, new_tokens], dim=1)\n",
    "\n",
    "            #Updating the beam scores to be used after this step in generation\n",
    "            beam_scores=new_top_scores.view(-1)\n",
    "\n",
    "            #Printing decoder_input_ids to see whats going on\n",
    "            #print(f\"Decoder input ids at step{step}:\\n{decoder_input_ids}\")\n",
    "\n",
    "            # Check for early stopping\n",
    "            if early_stopping:\n",
    "                # If all beams for all sequences are finished, stop decoding\n",
    "                if new_tokens[0]==tokenizer.eos_token_id: #if the top beam at the time generates the EOS token then break the loop\n",
    "                    break\n",
    "\n",
    "\n",
    "    # Reshape and return the best sequences\n",
    "    output_sequences = decoder_input_ids.view(original_batch_size, num_beams, -1) # creates a 3D tensor with original_batch_size number of block, and each block containing num_beam rows, and each rows has lenght=sequence_length\n",
    "\n",
    "    # Select the top beam for each batch\n",
    "    best_sequences = output_sequences[:, 0, :] #slices to create a 2D tensor of shape(original_batch_size, sequence_lenght) which selects first row of each block since the first row would be the one with highest total score.\n",
    "\n",
    "    return best_sequences\n",
    "\n",
    "\n",
    "def translate(dataset):\n",
    "    #tokenize the dataset at once because it is a small dataset and tokenization does not need to be done in batches\n",
    "    dataset_t = tokenizer(\n",
    "                [row[\"en\"] for row in dataset[\"translation\"]],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device) #we move the tensors to the device that our model is on\n",
    "\n",
    "    # Generate translation for batches of tokenized dataset and add them to translated_dataset list, we do this in batches to speed up the process by making use of the parallel procesing but we dont do it all at once because of possible memory constraints.\n",
    "    input_ids=dataset_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "    attention_mask=dataset_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "    translated_dataset=[] #The list of generated translation which will be returned from the function\n",
    "    for i in tqdm(range(0,input_ids.shape[0], generation_batch_size)):\n",
    "        batch_input_ids= input_ids[i:i+generation_batch_size] #slicing for tensors works in the same way as slicing for lists, so if i+generation_batch_size>input_ids.shape[0] then slicing will just return a tensor until the last element of the parent tensor.\n",
    "        batch_attention_mask=attention_mask[i:i+generation_batch_size]\n",
    "        outputs = scratch_beam_search(input_ids=batch_input_ids, attention_mask=batch_attention_mask, num_beams=4, early_stopping=True)\n",
    "        translated_batch=tokenizer.batch_decode(outputs, skip_special_tokens=True) # tokenizer.decode could also handle 2D tensor containing muliple token_ids but .batch_decode is said to handle it faster\n",
    "        translated_dataset.extend(translated_batch) # .extend adds each element from an iterable to a list, whereas if we used .append here it would have added the entire iterable to the list as a single element\n",
    "\n",
    "    return translated_dataset\n",
    "\n",
    "# Translate all EN sentences\n",
    "model_generated_list = translate(eval_dataset)\n",
    "\n",
    "\n",
    "#BLEU score for model generation\n",
    "bleu_score = sacrebleu.corpus_bleu(model_generated_list, reference_list_BLEU)\n",
    "print(f\"BLEU score for validation split using scratch_beam_search:{bleu_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Implementing Lexical Constraints for 4 Iterations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpy73KPzMcLZ"
   },
   "source": [
    "##### Plan for Implementing Lexical constraints:\n",
    "1. The baseline is ready for implementing constraints\n",
    "\n",
    "2. As you can see I can get the model_generated_list which contains the list of translated sequences for entire dataset by the translate function(which further calls the scratch_beam_search function).\n",
    "\n",
    "3. Now I define a contrained_translate function that tokenizes the model_generated_list and target_reference list.Then runs a loop in which it calls find_missing_tokens function to return the missing tokens for the sequence in current iteration. Then it calls constrained_beam_search to include the constraint in the translation for that sequence. Then it adds the translation of that sequence to the constrained_translated_list.\n",
    "\n",
    "4. Then We calculte the BLEU score for the all 4 iterations of constrained_translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1355274,
     "status": "ok",
     "timestamp": 1733478561308,
     "user": {
      "displayName": "Anupreet Singh Sidhu",
      "userId": "10595161356432160963"
     },
     "user_tz": 300
    },
    "id": "g90oW_BgMcLZ",
    "outputId": "dc5d3b63-a68f-4fb6-e25d-e2240806009c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2818/2818 [22:31<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Bleu score:BLEU = 27.48 57.4/33.3/21.3/14.0 (BP = 1.000 ratio = 1.012 hyp_len = 56568 ref_len = 55920) \n",
      "\n",
      " Constrained Bleu Score:BLEU = 26.34 56.7/32.2/20.2/13.0 (BP = 1.000 ratio = 1.005 hyp_len = 56172 ref_len = 55920)\n"
     ]
    }
   ],
   "source": [
    "## Implementing Lexical Constraints Once(First iteration)\n",
    "eval_dataset=dataset[\"validation\"]\n",
    "source_reference_list=[row[\"en\"] for row in eval_dataset[\"translation\"]] #A list of all english sentences in the dataset\n",
    "target_reference_list=[row[\"ru\"] for row in eval_dataset[\"translation\"]] #A list of all russian sentence in the dataset\n",
    "reference_list_BLEU=[[row[\"ru\"] for row in eval_dataset[\"translation\"]]]\n",
    "generation_batch_size=1 #increase or decrease as per memory available in models processing unit\n",
    "model.eval()# Putting the model in eval mode\n",
    "GBS_constraints=[[] for _ in range(len(source_reference_list))] # we make a list containing len(source_reference_list) empty lists where each empty list will contain contraints for each sequence in the future This will be list of lists that contain subsequences(lexical constraints) for each input sequence in eval_dataset,\n",
    "\n",
    "\"\"\"\n",
    "find_missing_tokens function returns the n-gram(subsequence) from reference_seq which is missing from translated_sequence,\n",
    "It looks for 3-grams, if none found then for 2-grams, if none found then 1-grams, and return None if not even 1-grams are found to be missing from translated_sequence\n",
    "\"\"\"\n",
    "def find_missing_tokens(translated_seq, reference_seq):\n",
    "    #This nested function returns a boolean value after checking if the subsequence is missing from the translated_seq\n",
    "    def is_subsequence_missing(subsequence, translated_seq, n):\n",
    "        #Goes over all n-grams(subsequences) in the translated_seq\n",
    "        for i in range(len(translated_seq) - n + 1):\n",
    "            #compares all n-grams in translated_seq and\n",
    "            if torch.equal(translated_seq[i:i + n], subsequence): #if one of the subsequences in translated_seq is equal to subsequence it returns false because it means subsequence is not missing from translated_seq\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Check for missing subsequences of in the order 3, 2, and 1 tokens\n",
    "    for n in range(3, 0, -1):\n",
    "        #Goes over all n-grams(subsequences) in the reference_seq\n",
    "        for i in range(len(reference_seq) - n + 1): #number of n-grams in a sequence=len_of_seq-n+1, here len(reference_seq) give the length of the reference sequence since it is a 1D tensor\n",
    "            subsequence = reference_seq[i:i + n] #choosing the current n-gram\n",
    "            if is_subsequence_missing(subsequence, translated_seq, n): #returns True if current n-gram is missing\n",
    "                return subsequence, i #basically returns the first missing n-gram we can find in the order of looking for 3,2,1-grams and the position of the first token of the n-gram in reference\n",
    "\n",
    "    # If no missing subsequences are found, return None\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def constrained_beam_search(input_ids, attention_mask, constraints, num_beams=4, max_length=128, early_stopping=True):\n",
    "    original_batch_size = input_ids.shape[0] #will be 1, because input_ids here is a 1D tensor in case we are only sending in 1 sequence from constrained_translate() to be translated in constrained_beam_search_ at a time.\n",
    "\n",
    "    #scaling original input to num_beams for beam search\n",
    "    input_ids = input_ids.repeat_interleave(num_beams, dim=0) #creating num_beam copies the each input sequence(one for each beam).\n",
    "    attention_mask = attention_mask.repeat_interleave(num_beams, dim=0) #doing the same thing again but for attention masks\n",
    "\n",
    "    # Creating tensor for storing outputs of current beams\n",
    "    decoder_input_ids = torch.full(  #making a 2D tensor of shape (original_batch_size * num_beams, 1) with each element equal to \"BOS\" token_id of the decoder/tokenizer\n",
    "                        (original_batch_size * num_beams, 1),\n",
    "                        model.config.decoder_start_token_id,\n",
    "                        dtype=torch.long,\n",
    "                        device=device\n",
    "                        )\n",
    "\n",
    "    # making a 1D tensor for storing scores of all beams of all sequences\n",
    "    beam_scores = torch.zeros(original_batch_size * num_beams, dtype=torch.float, device=device)\n",
    "\n",
    "    #Maintains a list of states for each beam\n",
    "    constraint_states = [[{\"active\": True, \"satisfied\": False, \"tokens_left\": constraint_info[\"token_ids\"].clone(), \"position\": constraint_info[\"position\"]} for constraint_info in constraints] for _ in range(original_batch_size * num_beams)] #list for each beam of all input sequences, each beam(row) has a list of constraints states\n",
    "                                                                                                                             #each constraint state is a dictionary with 3 keys- \"active\"(meaning it is actively looking to be fullfilled), \"satisfied\"(meaning it is satisfied), \"tokens_left\"(which contain a copy of the tokens to be included for that constraint in that beam)\n",
    "    Start=True ##initial Condition to be checked only once during the first forward pass                                                                                                                           #Active=True means the full constraint is pending or part of the constraint is pending\n",
    "    #Helper function for adding constraint token\n",
    "    def find_variable_in_constraints(step, constraints):\n",
    "        position_list=[]\n",
    "        for i, constraint_dict in enumerate(constraints):\n",
    "            if step == constraint_dict[\"position\"] and constraint_dict[\"active\"]==True: #Constraint must be active constraint\n",
    "                position_list.append(i)  # Return the index of the dictionary whose constraint position is equal to current decoding step\n",
    "        if position_list: #if their is anything in the position list\n",
    "            return position_list\n",
    "        return None     #Else return None\n",
    "    #Decoding Steps for generating the output\n",
    "    for step in range(max_length):#each decoding step, adding one token to a beam of decoder_input_ids\n",
    "\n",
    "        #Adding Constraint\n",
    "        constraint_num=find_variable_in_constraints(step, constraint_states[0]) #checking for position in first beam since all beams are updated together anyways, it will return constraints whose \"position\" is equal to the current step\n",
    "        if constraint_num is not None: #this step is equal to position of some constraint\n",
    "            #add constraint token to decoder_input_id\n",
    "            constraint_token_tensor=torch.full((original_batch_size * num_beams, 1), constraint_states[0][constraint_num[0]][\"tokens_left\"][0],dtype=torch.long, device=device)\n",
    "            \"\"\"\n",
    "            Explaination of above step:\n",
    "            constraint_states[0] gives us a list of dictionaries containing constraint info for all constraints of the first beam(since all constraints for all beams are updated together we can take any one, it doesnt matter)\n",
    "            then from the constraint_num list that has a list of indices for all constraints that have first token valid for this step, we choose the first one i.e.constraint_num[0](it wouldnt matter if you choose the second one too since the token for a specific position would be the same as they are all derived from the same reference translation)\n",
    "            Then we take the first token of that constraint and prepare a tensor to add it to the decoder input_ids\n",
    "            \"\"\"\n",
    "            #add the 1 token of constraint to beams of decoder_input_id\n",
    "            decoder_input_ids=torch.cat([decoder_input_ids, constraint_token_tensor], dim=1)\n",
    "\n",
    "\n",
    "            #update constraint_state for beams of the sequence\n",
    "            for i in range(original_batch_size*num_beams): #for all beams\n",
    "                for x in range(len(constraint_num)):  #for all constraints with position same as this step\n",
    "                    constraint_states[i][constraint_num[x]][\"position\"]+=1 #shift position 1 index forward for the next token\n",
    "                    constraint_states[i][constraint_num[x]][\"tokens_left\"]=constraint_states[i][constraint_num[x]][\"tokens_left\"][1:] #remove the token from the constraint that has been added to the decoder_input_id\n",
    "                    if len(constraint_states[i][constraint_num[x]][\"tokens_left\"])==0: #when there are no more tokens left in that constraint\n",
    "                        constraint_states[i][constraint_num[x]][\"active\"]=False #change the state of the constraint\n",
    "\n",
    "\n",
    "\n",
    "        #Generating token by use of logits and Scratch beam search\n",
    "        else: #this step is not a position of some constraint then we just generate using models logits\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=decoder_input_ids\n",
    "                    ) #This will be a dictionary like object containing a lot of things like hidden states,etc, but we are most interested in logits\n",
    "                \"\"\"\n",
    "                Talking about some important Tensor Operations:\n",
    "\n",
    "                Broadcasting:\n",
    "                In pytorch, a process called broadcasting happens just before an element wise operation that requires two tensors with same number of dimensions to align in shape.\n",
    "                Example Adding tensor A of shape(4,6,8) with tensor B of shape(4,1,1) then the scaler values in tensor B along the dimensions with only 1 element are duplicated to match the shape of the tensor A.\n",
    "                But broadcasting only happens when two tensors have same size of some dimensions and some dimensions of size 1.\n",
    "\n",
    "                tensor.view(size of dim=0, size of dim=1.....) is used to create a reshaped tensor with the same number of elements as the original tensor.\n",
    "                It can add new dimensions of a specific size as long as the number of element in the original tensor could be accomodated.\n",
    "                tensor.view(-1,10) tell us to accomodate 10 elements in dim=10 and infer what number of element to accomodate in dim=0 such that the total number of elements remain the same.\n",
    "\n",
    "                tensor.unsqueeze() is used to add a singleton dimension(dimension of size 1) at a specified index\n",
    "                tensor.unsequeeze(1) adds a singleton dimension at index=1\n",
    "\n",
    "                tensor.squeeze() is used remove a singleton dimension from a specified index\n",
    "\n",
    "                tensor.expand() is used to basically perform broadcasting on command, you would send in sizes of dimension of tensor which are capable of broadcasting and get a result.\n",
    "                This means that the size you send in should either be the same as sizes of dimension of original tensor OR in case of singleton dimension in original tensor you could send in a greater number to stretch along that dimension\n",
    "                tensor.expand(-1, 4) tells to keep dim=0 as is and basically stretch dim=1 to size 4, this is conditioned on the fact that the dimension to be stretched is of size 1 otherwise an error will occur\n",
    "\n",
    "                tensor.expand_as(target_tensor) is used to perform broadcasting on tensor conditioned on the fact that the size of target_tensor is suitable for broadcasting\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                # Creating a 2D tensor Logits\n",
    "                logits = outputs.logits[:, -1, :] #output.logits is a 3-D tensor of shape(original_batch_size*num_beam, current sequence length in decoder_input_ids, vocab size). where each element is the logit of each token_id of each sequence for each element of the vocabulary\n",
    "                                                #By slicing we choose the last token_id of each sequence it to get a 2-D tensor of shape(original_batch_size*num_beam, vocab_size) where each element is a logit of last token_id of each sequence element for each element of the vocabulary\n",
    "                #print(f\"Logits at step {step}:\\n{logits}\")\n",
    "                # Using Boolean masking to mark finished beams (those that already generated EOS)\n",
    "                #finished_mask = finished_beams.view(-1, 1).expand(-1, logits.size(-1))  # finished_mask will be a 2D of shape(original_batch_size * num_beams, vocab_size) with values of True/False repeated in each row(representing each beam) upto the vocab_size\n",
    "                #logits[finished_mask] = -float('inf')  # Making logits of lasts token for all elements of vocabulary for the finished beams negative infinity, By sending in boolean mask tensor of the same shape as the logits tensor the operation is applied to all logits elements that correspond with True in finished mask.\n",
    "                                                        #Making Logit for all words in vocabulary negative infinity means probability=0 after softmax, and log of that is a big negative value, when that big negative value is combined with beam_score to get next_score it ensures that these beams will now not come in the topk.\n",
    "                                                        #This ensures that only active beams (those not yet finished) are considered during the top-k selection.\n",
    "\n",
    "                # Creating 2D tensor log probabilities\n",
    "                log_probs = F.log_softmax(logits, dim=-1) #performs softmax on all logits normalizing them to probability of 0-1, then takes log of that so the highest probability is turend into the largest number(smallest negative value).\n",
    "                                                        #dim=-1 takes softmax along the last dimension(2nd dimension with size=vocab_size in this case) which means that normalization happens for each row(last token for each sequence of decoder_input_ids)\n",
    "                #print(f\"Log_probs at step {step}:\\n{log_probs}\")\n",
    "                #get top num_beams scores from each beam\n",
    "                total_top_log_probs, total_top_token_ids=torch.topk(log_probs, num_beams, dim=1) #give 2 tensors of shape(original_batch_size*num_beams, num_beams) where each row contains scores top num_beam scores for that row and the corresponding token_ids in descending\n",
    "                #print(f\"Topk log_probs at step {step}:\\n{total_top_log_probs}\")\n",
    "                #print(f\"Topk token_ids at step {step}:\\n{total_top_token_ids}\")\n",
    "\n",
    "                if Start==True:\n",
    "                    new_tokens=total_top_token_ids[::num_beams].reshape(-1,1) #using slicing with step value num_beam we take the top 4 tokens of a beam of a sequence and make a tensor containing new tokens for each beam, .reshape is used in place of .view() becauce we used slicing and so tensor is not contiguous in memory hence .view wont work\n",
    "                    decoder_input_ids=torch.cat([decoder_input_ids[torch.arange(original_batch_size*num_beams)],new_tokens], dim=1) #adding first 4 tokens to different beam of each sequence\n",
    "                    beam_scores=total_top_log_probs[::num_beams].reshape(-1) #slicing to make tensor of shape(original_batch_size,num_beams) and then we flatten it using .reshape(-1)\n",
    "                    Start=False\n",
    "                    #print(f\"Decoder_input_ids at step{step}:\\n{decoder_input_ids}\")\n",
    "                    #print(f\"Beam score at step{step}:\\n{beam_scores}\")\n",
    "\n",
    "                    continue\n",
    "\n",
    "                #calculate total scores for each beams top 4 tokens\n",
    "                total_top_score=total_top_log_probs+beam_scores.unsqueeze(1)\n",
    "                #print(f\"Total score of each topk token of each beam scores at step {step}:\\n{total_top_score}\")\n",
    "\n",
    "\n",
    "                #reshape that\n",
    "                reshaped_total_top_score=total_top_score.view(original_batch_size, num_beams*num_beams)\n",
    "\n",
    "                #Getting the best scores for this step in generation\n",
    "                new_top_scores,new_token_indices=torch.topk(reshaped_total_top_score,num_beams, dim=1) #gives 2 tensors of shape (original_batch_size, num_beam) containing the top num_beam score from all total_scores of one sequence in each row in descending order and their index in that row in total_top_score_reshaped\n",
    "                #print(f\"Total Top scores for new tokens of each sequence scores at step {step}:\\n{new_top_scores}\")\n",
    "                #calculating beam indices for the new tokens of each sequence with best score\n",
    "                beam_indices=new_token_indices//num_beams #elements of a row in this tensor will tell which token originally belonged to which beam for a sequence\n",
    "                #reshaping beam_indices and scaling indices of elements from row level to global level in the tensor\n",
    "                base_indices=torch.arange(original_batch_size,device=device).unsqueeze(1) * num_beams\n",
    "                scaled_beam_indices=beam_indices+base_indices #Tensor of shape shape (original_batch_size, num_beam)\n",
    "                flattened_scaled_beam_indices=scaled_beam_indices.view(-1) #making a 1D tensor of shape(original_batch_size*num_beam)\n",
    "\n",
    "                #By Advance Indexing using these scaled indices we will obtain sequences from decoder_input_ids of the respective beam that the indices point to\n",
    "                prev_decoder_inputs=decoder_input_ids[flattened_scaled_beam_indices]\n",
    "\n",
    "                #Getting the token_ids of the tokens to be added\n",
    "                reshaped_top_total_token_ids=total_top_token_ids.view(original_batch_size, num_beams*num_beams)\n",
    "                \"\"\"\n",
    "                Here each row will contain the token ids of the top scoring tokens of all beams of a sequence just like the structure of reshaped_total_top_score\n",
    "                Since new_token_indices tells the row index of the topk elements of that row, the same indices will also point to the token_ids in reshaped_top_total_token_ids\n",
    "                So we make a vector of shape (original_batch_size,1) where each element will help us point to a row indices in reshaped_top_total_token_ids\n",
    "                And then new_token_indices has column indices which help us point to columns of reshaped_top_total_token_ids for that row\n",
    "                in this case of Advacne indexing because we use two rows and columns we get a new tensor of shape of rows and columns used\n",
    "                \"\"\"\n",
    "\n",
    "                raw_new_tokens = reshaped_top_total_token_ids[torch.arange(original_batch_size,device=device).unsqueeze(1), new_token_indices] #the token_ids for the new tokens in a Tensor of shape(original_batch_size, num_beams)\n",
    "                new_tokens = raw_new_tokens.view(original_batch_size*num_beams,1) #2D Tensor of shape(original_batch_size*num_beam,1) made to be suited for concatenation to decoder_input_ids\n",
    "                decoder_input_ids=torch.cat([prev_decoder_inputs, new_tokens], dim=1)\n",
    "\n",
    "                #Updating the beam scores to be used after this step in generation\n",
    "                beam_scores=new_top_scores.view(-1)\n",
    "\n",
    "                #Printing decoder_input_ids to see whats going on\n",
    "                #print(f\"Decoder input ids at step{step}:\\n{decoder_input_ids}\")\n",
    "\n",
    "                # eos_mask = (new_tokens == tokenizer.eos_token_id) #creating 2D boolean of shape as new_tokens telling which tokens generated now are EOS by setting that value to True\n",
    "                # finished_beams = finished_beams | eos_mask.view(-1) #.view(-1)Flattens eos_mask to make it a 1D tensor of shape(original_batch_size*num_beams) same as finished_beams, then we perform an OR operation(|) to update finished beam to True for the beams that have just generated EOS tokens\n",
    "\n",
    "                # Check for early stopping\n",
    "                if early_stopping:\n",
    "                    # If all beams for all sequences are finished, stop decoding\n",
    "                    if new_tokens[0]==tokenizer.eos_token_id: #if the top beam at the time generates the EOS token then break the loop\n",
    "                        break\n",
    "\n",
    "\n",
    "    # Reshape and extract the best sequence for each input\n",
    "    output_sequences = decoder_input_ids.view(original_batch_size, num_beams, -1)\n",
    "    best_sequences = output_sequences[:, 0, :] #creates 2D tensor of shape(original_batch_size, sequence length)\n",
    "\n",
    "    return best_sequences\n",
    "\n",
    "\n",
    "# Requires a translated list(list containing MT hypothesis) of previous iteration and index of current iteration\n",
    "def constrained_translate(translated_list):\n",
    "    #Tokenizing translated list\n",
    "\n",
    "    translated_list_t = tokenizer(\n",
    "                translated_list,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "    translated_input_ids=translated_list_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "    translated_attention_mask=translated_list_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "\n",
    "    dataset_t = tokenizer(\n",
    "                source_reference_list,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "    source_input_ids=dataset_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "    source_attention_mask=dataset_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "\n",
    "\n",
    "    #Tokenizing target(russian) reference list\n",
    "    reference_list_t = tokenizer(\n",
    "            target_reference_list,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            ).to(device)\n",
    "\n",
    "    reference_input_ids = reference_list_t[\"input_ids\"]  # This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "    reference_attention_mask = reference_list_t[\"attention_mask\"]  # This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "\n",
    "    constrained_translated_list=[]\n",
    "\n",
    "\n",
    "    # Iterating through each translated sequence\n",
    "    for i in tqdm(range(len(translated_list))):\n",
    "        translated_seq_ids = translated_input_ids[i:i+1]  # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Single sequence from translated batch\n",
    "        reference_seq_ids = reference_input_ids[i:i+1]  # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Corresponding reference sequence\n",
    "        source_seq_ids= source_input_ids[i:i+1] # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Corresponding reference sequence\n",
    "        source_seq_attention_mask=source_attention_mask[i:i+1]\n",
    "\n",
    "\n",
    "        # Find phrases(subsequences) missing from translated_seq to add them as constraints\n",
    "        constraint, position = find_missing_tokens(translated_seq_ids[0], reference_seq_ids[0])#constraint will be a 1D array of token_ids, Sending in values of the one row in translated_seq_ids and reference_seq_ids in find_missing_tokens, so these are 1D tensors(imagine like a list of integers)\n",
    "                                                                                    #it could also return None in case of no mismatch\n",
    "\n",
    "        #In case there is no mismatch in translated and reference list just add the sequence from translated_list in the constrained_translated_list, and continue to next sequence\n",
    "        if constraint==None:\n",
    "            constrained_translated_list.append(translated_list[i]) #.append will add the string as a single element to constrained_translated_list, if we use .extend here it will treat the string as an iterable and add each of its elements(characters) to the constrained_translated_list\n",
    "            continue\n",
    "\n",
    "        constraint_info={\"token_ids\":constraint, \"position\":position}\n",
    "        #Add the constraint to the list of constraints for the current sequence\n",
    "        GBS_constraints[i].append(constraint_info)\n",
    "\n",
    "\n",
    "        #Generated translation using Grid_beam_search\n",
    "        constrained_seq_ids = constrained_beam_search(\n",
    "            input_ids=source_seq_ids,#sending input_ids of one sequence\n",
    "            attention_mask=source_seq_attention_mask, #sending attention mask of 1 sequence\n",
    "            constraints=GBS_constraints[i], #Sending constraints for that one sequence\n",
    "        )\n",
    "\n",
    "\n",
    "        constrained_translation = tokenizer.batch_decode(constrained_seq_ids, skip_special_tokens=True)\n",
    "\n",
    "        constrained_translated_list.extend(constrained_translation) #we use .extend here because it will add each element of the iterable constrained_translation to constrained_translated_list, constrained_translation could be a list of strings, but in this case is just a list containing one string\n",
    "\n",
    "    return constrained_translated_list\n",
    "\n",
    "\n",
    "#Getting the translation of input sequence by deriving constraints for each sequence of target_reference_list subsequences that are missing from model_generated_list\n",
    "constrained_translation_1=constrained_translate(model_generated_list)  #list of Constrained Translated Sequnces in Target Language\n",
    "\n",
    "\n",
    "#BLEU score for fist iteration\n",
    "Base_blue_score= sacrebleu.corpus_bleu(model_generated_list, reference_list_BLEU)\n",
    "print(f\"\\nBase Bleu score:{Base_blue_score} \")\n",
    "#Confirm number of elements in hypotheses list is equal to number of sequences in reference list)\n",
    "if len(constrained_translation_1)==len(reference_list_BLEU[0]):\n",
    "    constrained_bleu_score1= sacrebleu.corpus_bleu(constrained_translation_1, reference_list_BLEU)\n",
    "    print(f\"\\n Constrained Bleu Score for 1st iteration:{constrained_bleu_score1}\")\n",
    "\n",
    "else:\n",
    "    print(\"Mismatch in number of hypotheses and reference sequences\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1353540,
     "status": "ok",
     "timestamp": 1733479914845,
     "user": {
      "displayName": "Anupreet Singh Sidhu",
      "userId": "10595161356432160963"
     },
     "user_tz": 300
    },
    "id": "C8Rh7Oz4McLa",
    "outputId": "a80ff9bc-cc8b-4c1a-c42b-9e5cc329b567"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2818/2818 [22:29<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Bleu score:BLEU = 27.48 57.4/33.3/21.3/14.0 (BP = 1.000 ratio = 1.012 hyp_len = 56568 ref_len = 55920) \n",
      "\n",
      " Constrained Bleu Score:BLEU = 26.60 57.2/32.5/20.5/13.2 (BP = 1.000 ratio = 1.010 hyp_len = 56485 ref_len = 55920)\n"
     ]
    }
   ],
   "source": [
    "## Implementing Lexical Constraints (Second iteration)\n",
    "eval_dataset=dataset[\"validation\"]\n",
    "source_reference_list=[row[\"en\"] for row in eval_dataset[\"translation\"]] #A list of all english sentences in the dataset\n",
    "target_reference_list=[row[\"ru\"] for row in eval_dataset[\"translation\"]] #A list of all russian sentence in the dataset\n",
    "reference_list_BLEU=[[row[\"ru\"] for row in eval_dataset[\"translation\"]]]\n",
    "generation_batch_size=1 #increase or decrease as per memory available in models processing unit\n",
    "model.eval()# Putting the model in eval mode\n",
    "GBS_constraints_2=GBS_constraints\n",
    "\n",
    "\"\"\"\n",
    "find_missing_tokens function returns the n-gram(subsequence) from reference_seq which is missing from translated_sequence,\n",
    "It looks for 3-grams, if none found then for 2-grams, if none found then 1-grams, and return None if not even 1-grams are found to be missing from translated_sequence\n",
    "\"\"\"\n",
    "def find_missing_tokens(translated_seq, reference_seq):\n",
    "    #This nested function returns a boolean value after checking if the subsequence is missing from the translated_seq\n",
    "    def is_subsequence_missing(subsequence, translated_seq, n):\n",
    "        #Goes over all n-grams(subsequences) in the translated_seq\n",
    "        for i in range(len(translated_seq) - n + 1):\n",
    "            #compares all n-grams in translated_seq and\n",
    "            if torch.equal(translated_seq[i:i + n], subsequence): #if one of the subsequences in translated_seq is equal to subsequence it returns false because it means subsequence is not missing from translated_seq\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Check for missing subsequences of in the order 3, 2, and 1 tokens\n",
    "    for n in range(3, 0, -1):\n",
    "        #Goes over all n-grams(subsequences) in the reference_seq\n",
    "        for i in range(len(reference_seq) - n + 1): #number of n-grams in a sequence=len_of_seq-n+1, here len(reference_seq) give the length of the reference sequence since it is a 1D tensor\n",
    "            subsequence = reference_seq[i:i + n] #choosing the current n-gram\n",
    "            if is_subsequence_missing(subsequence, translated_seq, n): #returns True if current n-gram is missing\n",
    "                return subsequence, i #basically returns the first missing n-gram we can find in the order of looking for 3,2,1-grams and the position of the first token of the n-gram in reference\n",
    "\n",
    "    # If no missing subsequences are found, return None\n",
    "    return None, None\n",
    "\n",
    "def constrained_beam_search(input_ids, attention_mask, constraints, num_beams=4, max_length=128, early_stopping=True):\n",
    "    original_batch_size = input_ids.shape[0] #will be 1, because input_ids here is a 1D tensor in case we are only sending in 1 sequence from constrained_translate() to be translated in constrained_beam_search_ at a time.\n",
    "\n",
    "    #scaling original input to num_beams for beam search\n",
    "    input_ids = input_ids.repeat_interleave(num_beams, dim=0) #creating num_beam copies the each input sequence(one for each beam).\n",
    "    attention_mask = attention_mask.repeat_interleave(num_beams, dim=0) #doing the same thing again but for attention masks\n",
    "\n",
    "    # Creating tensor for storing outputs of current beams\n",
    "    decoder_input_ids = torch.full(  #making a 2D tensor of shape (original_batch_size * num_beams, 1) with each element equal to \"BOS\" token_id of the decoder/tokenizer\n",
    "                        (original_batch_size * num_beams, 1),\n",
    "                        model.config.decoder_start_token_id,\n",
    "                        dtype=torch.long,\n",
    "                        device=device\n",
    "                        )\n",
    "\n",
    "    # making a 1D tensor for storing scores of all beams of all sequences\n",
    "    beam_scores = torch.zeros(original_batch_size * num_beams, dtype=torch.float, device=device)\n",
    "\n",
    "    #Maintains a list of states for each beam\n",
    "    constraint_states = [[{\"active\": True, \"satisfied\": False, \"tokens_left\": constraint_info[\"token_ids\"].clone(), \"position\": constraint_info[\"position\"]} for constraint_info in constraints] for _ in range(original_batch_size * num_beams)] #list for each beam of all input sequences, each beam(row) has a list of constraints states\n",
    "                                                                                                                             #each constraint state is a dictionary with 3 keys- \"active\"(meaning it is actively looking to be fullfilled), \"satisfied\"(meaning it is satisfied), \"tokens_left\"(which contain a copy of the tokens to be included for that constraint in that beam)\n",
    "    Start=True ##initial Condition to be checked only once during the first forward pass                                                                                                                           #Active=True means the full constraint is pending or part of the constraint is pending\n",
    "    #Helper function for adding constraint token\n",
    "    def find_variable_in_constraints(step, constraints):\n",
    "        position_list=[]\n",
    "        for i, constraint_dict in enumerate(constraints):\n",
    "            if step == constraint_dict[\"position\"] and constraint_dict[\"active\"]==True: #Constraint must be active constraint\n",
    "                position_list.append(i)  # Return the index of the dictionary whose constraint position is equal to current decoding step\n",
    "        if position_list: #if their is anything in the position list\n",
    "            return position_list\n",
    "        return None     #Else return None\n",
    "    #Decoding Steps for generating the output\n",
    "    for step in range(max_length):#each decoding step, adding one token to a beam of decoder_input_ids\n",
    "\n",
    "        #Adding Constraint\n",
    "        constraint_num=find_variable_in_constraints(step, constraint_states[0]) #checking for position in first beam since all beams are updated together anyways, it will return constraints whose \"position\" is equal to the current step\n",
    "        if constraint_num is not None: #this step is equal to position of some constraint\n",
    "            #add constraint token to decoder_input_id\n",
    "            constraint_token_tensor=torch.full((original_batch_size * num_beams, 1), constraint_states[0][constraint_num[0]][\"tokens_left\"][0],dtype=torch.long, device=device)\n",
    "            \"\"\"\n",
    "            Explaination of above step:\n",
    "            constraint_states[0] gives us a list of dictionaries containing constraint info for all constraints of the first beam(since all constraints for all beams are updated together we can take any one, it doesnt matter)\n",
    "            then from the constraint_num list that has a list of indices for all constraints that have first token valid for this step, we choose the first one i.e.constraint_num[0](it wouldnt matter if you choose the second one too since the token for a specific position would be the same as they are all derived from the same reference translation)\n",
    "            Then we take the first token of that constraint and prepare a tensor to add it to the decoder input_ids\n",
    "            \"\"\"\n",
    "            #add the 1 token of constraint to beams of decoder_input_id\n",
    "            decoder_input_ids=torch.cat([decoder_input_ids, constraint_token_tensor], dim=1)\n",
    "\n",
    "\n",
    "            #update constraint_state for beams of the sequence\n",
    "            for i in range(original_batch_size*num_beams): #for all beams\n",
    "                for x in range(len(constraint_num)):  #for all constraints with position same as this step\n",
    "                    constraint_states[i][constraint_num[x]][\"position\"]+=1 #shift position 1 index forward for the next token\n",
    "                    constraint_states[i][constraint_num[x]][\"tokens_left\"]=constraint_states[i][constraint_num[x]][\"tokens_left\"][1:] #remove the token from the constraint that has been added to the decoder_input_id\n",
    "                    if len(constraint_states[i][constraint_num[x]][\"tokens_left\"])==0: #when there are no more tokens left in that constraint\n",
    "                        constraint_states[i][constraint_num[x]][\"active\"]=False #change the state of the constraint\n",
    "\n",
    "\n",
    "\n",
    "        #Generating token by use of logits and Scratch beam search\n",
    "        else: #this step is not a position of some constraint then we just generate using models logits\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=decoder_input_ids\n",
    "                    ) #This will be a dictionary like object containing a lot of things like hidden states,etc, but we are most interested in logits\n",
    "                \"\"\"\n",
    "                Talking about some important Tensor Operations:\n",
    "\n",
    "                Broadcasting:\n",
    "                In pytorch, a process called broadcasting happens just before an element wise operation that requires two tensors with same number of dimensions to align in shape.\n",
    "                Example Adding tensor A of shape(4,6,8) with tensor B of shape(4,1,1) then the scaler values in tensor B along the dimensions with only 1 element are duplicated to match the shape of the tensor A.\n",
    "                But broadcasting only happens when two tensors have same size of some dimensions and some dimensions of size 1.\n",
    "\n",
    "                tensor.view(size of dim=0, size of dim=1.....) is used to create a reshaped tensor with the same number of elements as the original tensor.\n",
    "                It can add new dimensions of a specific size as long as the number of element in the original tensor could be accomodated.\n",
    "                tensor.view(-1,10) tell us to accomodate 10 elements in dim=10 and infer what number of element to accomodate in dim=0 such that the total number of elements remain the same.\n",
    "\n",
    "                tensor.unsqueeze() is used to add a singleton dimension(dimension of size 1) at a specified index\n",
    "                tensor.unsequeeze(1) adds a singleton dimension at index=1\n",
    "\n",
    "                tensor.squeeze() is used remove a singleton dimension from a specified index\n",
    "\n",
    "                tensor.expand() is used to basically perform broadcasting on command, you would send in sizes of dimension of tensor which are capable of broadcasting and get a result.\n",
    "                This means that the size you send in should either be the same as sizes of dimension of original tensor OR in case of singleton dimension in original tensor you could send in a greater number to stretch along that dimension\n",
    "                tensor.expand(-1, 4) tells to keep dim=0 as is and basically stretch dim=1 to size 4, this is conditioned on the fact that the dimension to be stretched is of size 1 otherwise an error will occur\n",
    "\n",
    "                tensor.expand_as(target_tensor) is used to perform broadcasting on tensor conditioned on the fact that the size of target_tensor is suitable for broadcasting\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                # Creating a 2D tensor Logits\n",
    "                logits = outputs.logits[:, -1, :] #output.logits is a 3-D tensor of shape(original_batch_size*num_beam, current sequence length in decoder_input_ids, vocab size). where each element is the logit of each token_id of each sequence for each element of the vocabulary\n",
    "                                                #By slicing we choose the last token_id of each sequence it to get a 2-D tensor of shape(original_batch_size*num_beam, vocab_size) where each element is a logit of last token_id of each sequence element for each element of the vocabulary\n",
    "                #print(f\"Logits at step {step}:\\n{logits}\")\n",
    "                # Using Boolean masking to mark finished beams (those that already generated EOS)\n",
    "                #finished_mask = finished_beams.view(-1, 1).expand(-1, logits.size(-1))  # finished_mask will be a 2D of shape(original_batch_size * num_beams, vocab_size) with values of True/False repeated in each row(representing each beam) upto the vocab_size\n",
    "                #logits[finished_mask] = -float('inf')  # Making logits of lasts token for all elements of vocabulary for the finished beams negative infinity, By sending in boolean mask tensor of the same shape as the logits tensor the operation is applied to all logits elements that correspond with True in finished mask.\n",
    "                                                        #Making Logit for all words in vocabulary negative infinity means probability=0 after softmax, and log of that is a big negative value, when that big negative value is combined with beam_score to get next_score it ensures that these beams will now not come in the topk.\n",
    "                                                        #This ensures that only active beams (those not yet finished) are considered during the top-k selection.\n",
    "\n",
    "                # Creating 2D tensor log probabilities\n",
    "                log_probs = F.log_softmax(logits, dim=-1) #performs softmax on all logits normalizing them to probability of 0-1, then takes log of that so the highest probability is turend into the largest number(smallest negative value).\n",
    "                                                        #dim=-1 takes softmax along the last dimension(2nd dimension with size=vocab_size in this case) which means that normalization happens for each row(last token for each sequence of decoder_input_ids)\n",
    "                #print(f\"Log_probs at step {step}:\\n{log_probs}\")\n",
    "                #get top num_beams scores from each beam\n",
    "                total_top_log_probs, total_top_token_ids=torch.topk(log_probs, num_beams, dim=1) #give 2 tensors of shape(original_batch_size*num_beams, num_beams) where each row contains scores top num_beam scores for that row and the corresponding token_ids in descending\n",
    "                #print(f\"Topk log_probs at step {step}:\\n{total_top_log_probs}\")\n",
    "                #print(f\"Topk token_ids at step {step}:\\n{total_top_token_ids}\")\n",
    "\n",
    "                if Start==True:\n",
    "                    new_tokens=total_top_token_ids[::num_beams].reshape(-1,1) #using slicing with step value num_beam we take the top 4 tokens of a beam of a sequence and make a tensor containing new tokens for each beam, .reshape is used in place of .view() becauce we used slicing and so tensor is not contiguous in memory hence .view wont work\n",
    "                    decoder_input_ids=torch.cat([decoder_input_ids[torch.arange(original_batch_size*num_beams)],new_tokens], dim=1) #adding first 4 tokens to different beam of each sequence\n",
    "                    beam_scores=total_top_log_probs[::num_beams].reshape(-1) #slicing to make tensor of shape(original_batch_size,num_beams) and then we flatten it using .reshape(-1)\n",
    "                    Start=False\n",
    "                    #print(f\"Decoder_input_ids at step{step}:\\n{decoder_input_ids}\")\n",
    "                    #print(f\"Beam score at step{step}:\\n{beam_scores}\")\n",
    "\n",
    "                    continue\n",
    "\n",
    "                #calculate total scores for each beams top 4 tokens\n",
    "                total_top_score=total_top_log_probs+beam_scores.unsqueeze(1)\n",
    "                #print(f\"Total score of each topk token of each beam scores at step {step}:\\n{total_top_score}\")\n",
    "\n",
    "\n",
    "                #reshape that\n",
    "                reshaped_total_top_score=total_top_score.view(original_batch_size, num_beams*num_beams)\n",
    "\n",
    "                #Getting the best scores for this step in generation\n",
    "                new_top_scores,new_token_indices=torch.topk(reshaped_total_top_score,num_beams, dim=1) #gives 2 tensors of shape (original_batch_size, num_beam) containing the top num_beam score from all total_scores of one sequence in each row in descending order and their index in that row in total_top_score_reshaped\n",
    "                #print(f\"Total Top scores for new tokens of each sequence scores at step {step}:\\n{new_top_scores}\")\n",
    "                #calculating beam indices for the new tokens of each sequence with best score\n",
    "                beam_indices=new_token_indices//num_beams #elements of a row in this tensor will tell which token originally belonged to which beam for a sequence\n",
    "                #reshaping beam_indices and scaling indices of elements from row level to global level in the tensor\n",
    "                base_indices=torch.arange(original_batch_size,device=device).unsqueeze(1) * num_beams\n",
    "                scaled_beam_indices=beam_indices+base_indices #Tensor of shape shape (original_batch_size, num_beam)\n",
    "                flattened_scaled_beam_indices=scaled_beam_indices.view(-1) #making a 1D tensor of shape(original_batch_size*num_beam)\n",
    "\n",
    "                #By Advance Indexing using these scaled indices we will obtain sequences from decoder_input_ids of the respective beam that the indices point to\n",
    "                prev_decoder_inputs=decoder_input_ids[flattened_scaled_beam_indices]\n",
    "\n",
    "                #Getting the token_ids of the tokens to be added\n",
    "                reshaped_top_total_token_ids=total_top_token_ids.view(original_batch_size, num_beams*num_beams)\n",
    "                \"\"\"\n",
    "                Here each row will contain the token ids of the top scoring tokens of all beams of a sequence just like the structure of reshaped_total_top_score\n",
    "                Since new_token_indices tells the row index of the topk elements of that row, the same indices will also point to the token_ids in reshaped_top_total_token_ids\n",
    "                So we make a vector of shape (original_batch_size,1) where each element will help us point to a row indices in reshaped_top_total_token_ids\n",
    "                And then new_token_indices has column indices which help us point to columns of reshaped_top_total_token_ids for that row\n",
    "                in this case of Advacne indexing because we use two rows and columns we get a new tensor of shape of rows and columns used\n",
    "                \"\"\"\n",
    "\n",
    "                raw_new_tokens = reshaped_top_total_token_ids[torch.arange(original_batch_size,device=device).unsqueeze(1), new_token_indices] #the token_ids for the new tokens in a Tensor of shape(original_batch_size, num_beams)\n",
    "                new_tokens = raw_new_tokens.view(original_batch_size*num_beams,1) #2D Tensor of shape(original_batch_size*num_beam,1) made to be suited for concatenation to decoder_input_ids\n",
    "                decoder_input_ids=torch.cat([prev_decoder_inputs, new_tokens], dim=1)\n",
    "\n",
    "                #Updating the beam scores to be used after this step in generation\n",
    "                beam_scores=new_top_scores.view(-1)\n",
    "\n",
    "                #Printing decoder_input_ids to see whats going on\n",
    "                #print(f\"Decoder input ids at step{step}:\\n{decoder_input_ids}\")\n",
    "\n",
    "                # eos_mask = (new_tokens == tokenizer.eos_token_id) #creating 2D boolean of shape as new_tokens telling which tokens generated now are EOS by setting that value to True\n",
    "                # finished_beams = finished_beams | eos_mask.view(-1) #.view(-1)Flattens eos_mask to make it a 1D tensor of shape(original_batch_size*num_beams) same as finished_beams, then we perform an OR operation(|) to update finished beam to True for the beams that have just generated EOS tokens\n",
    "\n",
    "                # Check for early stopping\n",
    "                if early_stopping:\n",
    "                    # If all beams for all sequences are finished, stop decoding\n",
    "                    if new_tokens[0]==tokenizer.eos_token_id: #if the top beam at the time generates the EOS token then break the loop\n",
    "                        break\n",
    "\n",
    "\n",
    "    # Reshape and extract the best sequence for each input\n",
    "    output_sequences = decoder_input_ids.view(original_batch_size, num_beams, -1)\n",
    "    best_sequences = output_sequences[:, 0, :] #creates 2D tensor of shape(original_batch_size, sequence length)\n",
    "\n",
    "    return best_sequences\n",
    "\n",
    "\n",
    "# Requires a translated list(list containing MT hypothesis) of previous iteration and index of current iteration\n",
    "def constrained_translate(translated_list):\n",
    "    #Tokenizing translated list\n",
    "\n",
    "    translated_list_t = tokenizer(\n",
    "                translated_list,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "    translated_input_ids=translated_list_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "    translated_attention_mask=translated_list_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "\n",
    "    dataset_t = tokenizer(\n",
    "                source_reference_list,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "    source_input_ids=dataset_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "    source_attention_mask=dataset_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "\n",
    "\n",
    "    #Tokenizing target(russian) reference list\n",
    "    reference_list_t = tokenizer(\n",
    "            target_reference_list,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            ).to(device)\n",
    "\n",
    "    reference_input_ids = reference_list_t[\"input_ids\"]  # This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "    reference_attention_mask = reference_list_t[\"attention_mask\"]  # This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "\n",
    "    constrained_translated_list=[]\n",
    "\n",
    "\n",
    "    # Iterating through each translated sequence\n",
    "    for i in tqdm(range(len(translated_list))):\n",
    "        translated_seq_ids = translated_input_ids[i:i+1]  # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Single sequence from translated batch\n",
    "        reference_seq_ids = reference_input_ids[i:i+1]  # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Corresponding reference sequence\n",
    "        source_seq_ids= source_input_ids[i:i+1] # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Corresponding reference sequence\n",
    "        source_seq_attention_mask=source_attention_mask[i:i+1]\n",
    "\n",
    "\n",
    "        # Find phrases(subsequences) missing from translated_seq to add them as constraints\n",
    "        constraint, position = find_missing_tokens(translated_seq_ids[0], reference_seq_ids[0])#constraint will be a 1D array of token_ids, Sending in values of the one row in translated_seq_ids and reference_seq_ids in find_missing_tokens, so these are 1D tensors(imagine like a list of integers)\n",
    "                                                                                    #it could also return None in case of no mismatch\n",
    "\n",
    "        #In case there is no mismatch in translated and reference list just add the sequence from translated_list in the constrained_translated_list, and continue to next sequence\n",
    "        if constraint==None:\n",
    "            constrained_translated_list.append(translated_list[i]) #.append will add the string as a single element to constrained_translated_list, if we use .extend here it will treat the string as an iterable and add each of its elements(characters) to the constrained_translated_list\n",
    "\n",
    "            continue\n",
    "\n",
    "        constraint_info={\"token_ids\":constraint, \"position\":position}\n",
    "        #Add the constraint to the list of constraints for the current sequence\n",
    "        GBS_constraints_2[i].append(constraint_info)\n",
    "\n",
    "\n",
    "        #Generated translation using Grid_beam_search\n",
    "        constrained_seq_ids = constrained_beam_search(\n",
    "            input_ids=source_seq_ids,#sending input_ids of one sequence\n",
    "            attention_mask=source_seq_attention_mask, #sending attention mask of 1 sequence\n",
    "            constraints=GBS_constraints_2[i], #Sending constraints for that one sequence\n",
    "        )\n",
    "\n",
    "\n",
    "        constrained_translation = tokenizer.batch_decode(constrained_seq_ids, skip_special_tokens=True)\n",
    "        constrained_translated_list.extend(constrained_translation) #we use .extend here because it will add each element of the iterable constrained_translation to constrained_translated_list, constrained_translation could be a list of strings, but in this case is just a list containing one string\n",
    "\n",
    "\n",
    "    return constrained_translated_list\n",
    "\n",
    "\n",
    "#Getting the translation of input sequence by deriving constraints for each sequence of target_reference_list subsequences that are missing from model_generated_list\n",
    "constrained_translation_2=constrained_translate(constrained_translation_1)  #list of Constrained Translated Sequnces in Target Language\n",
    "\n",
    "\n",
    "#BLEU score for second iteration\n",
    "Base_blue_score= sacrebleu.corpus_bleu(model_generated_list, reference_list_BLEU)\n",
    "print(f\"\\nBase Bleu score:{Base_blue_score} \")\n",
    "#Confirm number of elements in hypotheses list is equal to number of sequences in reference list)\n",
    "if len(constrained_translation_2)==len(reference_list_BLEU[0]):\n",
    "    constrained_bleu_score2= sacrebleu.corpus_bleu(constrained_translation_2, reference_list_BLEU)\n",
    "    print(f\"\\n Constrained Bleu Score for 2nd iteration:{constrained_bleu_score2}\")\n",
    "\n",
    "else:\n",
    "    print(\"Mismatch in number of hypotheses and reference sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1352846,
     "status": "ok",
     "timestamp": 1733481267687,
     "user": {
      "displayName": "Anupreet Singh Sidhu",
      "userId": "10595161356432160963"
     },
     "user_tz": 300
    },
    "id": "6TwUB_8WMcLb",
    "outputId": "4b55eac0-a605-476e-f5ac-a99228210d5b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2818/2818 [22:28<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Bleu score:BLEU = 27.48 57.4/33.3/21.3/14.0 (BP = 1.000 ratio = 1.012 hyp_len = 56568 ref_len = 55920) \n",
      "\n",
      " Constrained Bleu Score:BLEU = 26.47 57.1/32.3/20.4/13.1 (BP = 1.000 ratio = 1.009 hyp_len = 56443 ref_len = 55920)\n"
     ]
    }
   ],
   "source": [
    "## Implementing Lexical Constraints (Third iteration)\n",
    "eval_dataset=dataset[\"validation\"]\n",
    "source_reference_list=[row[\"en\"] for row in eval_dataset[\"translation\"]] #A list of all english sentences in the dataset\n",
    "target_reference_list=[row[\"ru\"] for row in eval_dataset[\"translation\"]] #A list of all russian sentence in the dataset\n",
    "reference_list_BLEU=[[row[\"ru\"] for row in eval_dataset[\"translation\"]]]\n",
    "generation_batch_size=1 #increase or decrease as per memory available in models processing unit\n",
    "model.eval()# Putting the model in eval mode\n",
    "GBS_constraints_3=GBS_constraints_2\n",
    "\"\"\"\n",
    "find_missing_tokens function returns the n-gram(subsequence) from reference_seq which is missing from translated_sequence,\n",
    "It looks for 3-grams, if none found then for 2-grams, if none found then 1-grams, and return None if not even 1-grams are found to be missing from translated_sequence\n",
    "\"\"\"\n",
    "def find_missing_tokens(translated_seq, reference_seq):\n",
    "    #This nested function returns a boolean value after checking if the subsequence is missing from the translated_seq\n",
    "    def is_subsequence_missing(subsequence, translated_seq, n):\n",
    "        #Goes over all n-grams(subsequences) in the translated_seq\n",
    "        for i in range(len(translated_seq) - n + 1):\n",
    "            #compares all n-grams in translated_seq and\n",
    "            if torch.equal(translated_seq[i:i + n], subsequence): #if one of the subsequences in translated_seq is equal to subsequence it returns false because it means subsequence is not missing from translated_seq\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Check for missing subsequences of in the order 3, 2, and 1 tokens\n",
    "    for n in range(3, 0, -1):\n",
    "        #Goes over all n-grams(subsequences) in the reference_seq\n",
    "        for i in range(len(reference_seq) - n + 1): #number of n-grams in a sequence=len_of_seq-n+1, here len(reference_seq) give the length of the reference sequence since it is a 1D tensor\n",
    "            subsequence = reference_seq[i:i + n] #choosing the current n-gram\n",
    "            if is_subsequence_missing(subsequence, translated_seq, n): #returns True if current n-gram is missing\n",
    "                return subsequence, i #basically returns the first missing n-gram we can find in the order of looking for 3,2,1-grams and the position of the first token of the n-gram in reference\n",
    "\n",
    "    # If no missing subsequences are found, return None\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def constrained_beam_search(input_ids, attention_mask, constraints, num_beams=4, max_length=128, early_stopping=True):\n",
    "    original_batch_size = input_ids.shape[0] #will be 1, because input_ids here is a 1D tensor in case we are only sending in 1 sequence from constrained_translate() to be translated in constrained_beam_search_ at a time.\n",
    "\n",
    "    #scaling original input to num_beams for beam search\n",
    "    input_ids = input_ids.repeat_interleave(num_beams, dim=0) #creating num_beam copies the each input sequence(one for each beam).\n",
    "    attention_mask = attention_mask.repeat_interleave(num_beams, dim=0) #doing the same thing again but for attention masks\n",
    "\n",
    "    # Creating tensor for storing outputs of current beams\n",
    "    decoder_input_ids = torch.full(  #making a 2D tensor of shape (original_batch_size * num_beams, 1) with each element equal to \"BOS\" token_id of the decoder/tokenizer\n",
    "                        (original_batch_size * num_beams, 1),\n",
    "                        model.config.decoder_start_token_id,\n",
    "                        dtype=torch.long,\n",
    "                        device=device\n",
    "                        )\n",
    "\n",
    "    # making a 1D tensor for storing scores of all beams of all sequences\n",
    "    beam_scores = torch.zeros(original_batch_size * num_beams, dtype=torch.float, device=device)\n",
    "\n",
    "    #Maintains a list of states for each beam\n",
    "    constraint_states = [[{\"active\": True, \"satisfied\": False, \"tokens_left\": constraint_info[\"token_ids\"].clone(), \"position\": constraint_info[\"position\"]} for constraint_info in constraints] for _ in range(original_batch_size * num_beams)] #list for each beam of all input sequences, each beam(row) has a list of constraints states\n",
    "                                                                                                                             #each constraint state is a dictionary with 3 keys- \"active\"(meaning it is actively looking to be fullfilled), \"satisfied\"(meaning it is satisfied), \"tokens_left\"(which contain a copy of the tokens to be included for that constraint in that beam)\n",
    "    Start=True ##initial Condition to be checked only once during the first forward pass                                                                                                                           #Active=True means the full constraint is pending or part of the constraint is pending\n",
    "    #Helper function for adding constraint token\n",
    "    def find_variable_in_constraints(step, constraints):\n",
    "        position_list=[]\n",
    "        for i, constraint_dict in enumerate(constraints):\n",
    "            if step == constraint_dict[\"position\"] and constraint_dict[\"active\"]==True: #Constraint must be active constraint\n",
    "                position_list.append(i)  # Return the index of the dictionary whose constraint position is equal to current decoding step\n",
    "        if position_list: #if their is anything in the position list\n",
    "            return position_list\n",
    "        return None     #Else return None\n",
    "    #Decoding Steps for generating the output\n",
    "    for step in range(max_length):#each decoding step, adding one token to a beam of decoder_input_ids\n",
    "\n",
    "        #Adding Constraint\n",
    "        constraint_num=find_variable_in_constraints(step, constraint_states[0]) #checking for position in first beam since all beams are updated together anyways, it will return constraints whose \"position\" is equal to the current step\n",
    "        if constraint_num is not None: #this step is equal to position of some constraint\n",
    "            #add constraint token to decoder_input_id\n",
    "            constraint_token_tensor=torch.full((original_batch_size * num_beams, 1), constraint_states[0][constraint_num[0]][\"tokens_left\"][0],dtype=torch.long, device=device)\n",
    "            \"\"\"\n",
    "            Explaination of above step:\n",
    "            constraint_states[0] gives us a list of dictionaries containing constraint info for all constraints of the first beam(since all constraints for all beams are updated together we can take any one, it doesnt matter)\n",
    "            then from the constraint_num list that has a list of indices for all constraints that have first token valid for this step, we choose the first one i.e.constraint_num[0](it wouldnt matter if you choose the second one too since the token for a specific position would be the same as they are all derived from the same reference translation)\n",
    "            Then we take the first token of that constraint and prepare a tensor to add it to the decoder input_ids\n",
    "            \"\"\"\n",
    "            #add the 1 token of constraint to beams of decoder_input_id\n",
    "            decoder_input_ids=torch.cat([decoder_input_ids, constraint_token_tensor], dim=1)\n",
    "\n",
    "\n",
    "            #update constraint_state for beams of the sequence\n",
    "            for i in range(original_batch_size*num_beams): #for all beams\n",
    "                for x in range(len(constraint_num)):  #for all constraints with position same as this step\n",
    "                    constraint_states[i][constraint_num[x]][\"position\"]+=1 #shift position 1 index forward for the next token\n",
    "                    constraint_states[i][constraint_num[x]][\"tokens_left\"]=constraint_states[i][constraint_num[x]][\"tokens_left\"][1:] #remove the token from the constraint that has been added to the decoder_input_id\n",
    "                    if len(constraint_states[i][constraint_num[x]][\"tokens_left\"])==0: #when there are no more tokens left in that constraint\n",
    "                        constraint_states[i][constraint_num[x]][\"active\"]=False #change the state of the constraint\n",
    "\n",
    "\n",
    "\n",
    "        #Generating token by use of logits and Scratch beam search\n",
    "        else: #this step is not a position of some constraint then we just generate using models logits\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=decoder_input_ids\n",
    "                    ) #This will be a dictionary like object containing a lot of things like hidden states,etc, but we are most interested in logits\n",
    "                \"\"\"\n",
    "                Talking about some important Tensor Operations:\n",
    "\n",
    "                Broadcasting:\n",
    "                In pytorch, a process called broadcasting happens just before an element wise operation that requires two tensors with same number of dimensions to align in shape.\n",
    "                Example Adding tensor A of shape(4,6,8) with tensor B of shape(4,1,1) then the scaler values in tensor B along the dimensions with only 1 element are duplicated to match the shape of the tensor A.\n",
    "                But broadcasting only happens when two tensors have same size of some dimensions and some dimensions of size 1.\n",
    "\n",
    "                tensor.view(size of dim=0, size of dim=1.....) is used to create a reshaped tensor with the same number of elements as the original tensor.\n",
    "                It can add new dimensions of a specific size as long as the number of element in the original tensor could be accomodated.\n",
    "                tensor.view(-1,10) tell us to accomodate 10 elements in dim=10 and infer what number of element to accomodate in dim=0 such that the total number of elements remain the same.\n",
    "\n",
    "                tensor.unsqueeze() is used to add a singleton dimension(dimension of size 1) at a specified index\n",
    "                tensor.unsequeeze(1) adds a singleton dimension at index=1\n",
    "\n",
    "                tensor.squeeze() is used remove a singleton dimension from a specified index\n",
    "\n",
    "                tensor.expand() is used to basically perform broadcasting on command, you would send in sizes of dimension of tensor which are capable of broadcasting and get a result.\n",
    "                This means that the size you send in should either be the same as sizes of dimension of original tensor OR in case of singleton dimension in original tensor you could send in a greater number to stretch along that dimension\n",
    "                tensor.expand(-1, 4) tells to keep dim=0 as is and basically stretch dim=1 to size 4, this is conditioned on the fact that the dimension to be stretched is of size 1 otherwise an error will occur\n",
    "\n",
    "                tensor.expand_as(target_tensor) is used to perform broadcasting on tensor conditioned on the fact that the size of target_tensor is suitable for broadcasting\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                # Creating a 2D tensor Logits\n",
    "                logits = outputs.logits[:, -1, :] #output.logits is a 3-D tensor of shape(original_batch_size*num_beam, current sequence length in decoder_input_ids, vocab size). where each element is the logit of each token_id of each sequence for each element of the vocabulary\n",
    "                                                #By slicing we choose the last token_id of each sequence it to get a 2-D tensor of shape(original_batch_size*num_beam, vocab_size) where each element is a logit of last token_id of each sequence element for each element of the vocabulary\n",
    "                #print(f\"Logits at step {step}:\\n{logits}\")\n",
    "                # Using Boolean masking to mark finished beams (those that already generated EOS)\n",
    "                #finished_mask = finished_beams.view(-1, 1).expand(-1, logits.size(-1))  # finished_mask will be a 2D of shape(original_batch_size * num_beams, vocab_size) with values of True/False repeated in each row(representing each beam) upto the vocab_size\n",
    "                #logits[finished_mask] = -float('inf')  # Making logits of lasts token for all elements of vocabulary for the finished beams negative infinity, By sending in boolean mask tensor of the same shape as the logits tensor the operation is applied to all logits elements that correspond with True in finished mask.\n",
    "                                                        #Making Logit for all words in vocabulary negative infinity means probability=0 after softmax, and log of that is a big negative value, when that big negative value is combined with beam_score to get next_score it ensures that these beams will now not come in the topk.\n",
    "                                                        #This ensures that only active beams (those not yet finished) are considered during the top-k selection.\n",
    "\n",
    "                # Creating 2D tensor log probabilities\n",
    "                log_probs = F.log_softmax(logits, dim=-1) #performs softmax on all logits normalizing them to probability of 0-1, then takes log of that so the highest probability is turend into the largest number(smallest negative value).\n",
    "                                                        #dim=-1 takes softmax along the last dimension(2nd dimension with size=vocab_size in this case) which means that normalization happens for each row(last token for each sequence of decoder_input_ids)\n",
    "                #print(f\"Log_probs at step {step}:\\n{log_probs}\")\n",
    "                #get top num_beams scores from each beam\n",
    "                total_top_log_probs, total_top_token_ids=torch.topk(log_probs, num_beams, dim=1) #give 2 tensors of shape(original_batch_size*num_beams, num_beams) where each row contains scores top num_beam scores for that row and the corresponding token_ids in descending\n",
    "                #print(f\"Topk log_probs at step {step}:\\n{total_top_log_probs}\")\n",
    "                #print(f\"Topk token_ids at step {step}:\\n{total_top_token_ids}\")\n",
    "\n",
    "                if Start==True:\n",
    "                    new_tokens=total_top_token_ids[::num_beams].reshape(-1,1) #using slicing with step value num_beam we take the top 4 tokens of a beam of a sequence and make a tensor containing new tokens for each beam, .reshape is used in place of .view() becauce we used slicing and so tensor is not contiguous in memory hence .view wont work\n",
    "                    decoder_input_ids=torch.cat([decoder_input_ids[torch.arange(original_batch_size*num_beams)],new_tokens], dim=1) #adding first 4 tokens to different beam of each sequence\n",
    "                    beam_scores=total_top_log_probs[::num_beams].reshape(-1) #slicing to make tensor of shape(original_batch_size,num_beams) and then we flatten it using .reshape(-1)\n",
    "                    Start=False\n",
    "                    #print(f\"Decoder_input_ids at step{step}:\\n{decoder_input_ids}\")\n",
    "                    #print(f\"Beam score at step{step}:\\n{beam_scores}\")\n",
    "\n",
    "                    continue\n",
    "\n",
    "                #calculate total scores for each beams top 4 tokens\n",
    "                total_top_score=total_top_log_probs+beam_scores.unsqueeze(1)\n",
    "                #print(f\"Total score of each topk token of each beam scores at step {step}:\\n{total_top_score}\")\n",
    "\n",
    "\n",
    "                #reshape that\n",
    "                reshaped_total_top_score=total_top_score.view(original_batch_size, num_beams*num_beams)\n",
    "\n",
    "                #Getting the best scores for this step in generation\n",
    "                new_top_scores,new_token_indices=torch.topk(reshaped_total_top_score,num_beams, dim=1) #gives 2 tensors of shape (original_batch_size, num_beam) containing the top num_beam score from all total_scores of one sequence in each row in descending order and their index in that row in total_top_score_reshaped\n",
    "                #print(f\"Total Top scores for new tokens of each sequence scores at step {step}:\\n{new_top_scores}\")\n",
    "                #calculating beam indices for the new tokens of each sequence with best score\n",
    "                beam_indices=new_token_indices//num_beams #elements of a row in this tensor will tell which token originally belonged to which beam for a sequence\n",
    "                #reshaping beam_indices and scaling indices of elements from row level to global level in the tensor\n",
    "                base_indices=torch.arange(original_batch_size,device=device).unsqueeze(1) * num_beams\n",
    "                scaled_beam_indices=beam_indices+base_indices #Tensor of shape shape (original_batch_size, num_beam)\n",
    "                flattened_scaled_beam_indices=scaled_beam_indices.view(-1) #making a 1D tensor of shape(original_batch_size*num_beam)\n",
    "\n",
    "                #By Advance Indexing using these scaled indices we will obtain sequences from decoder_input_ids of the respective beam that the indices point to\n",
    "                prev_decoder_inputs=decoder_input_ids[flattened_scaled_beam_indices]\n",
    "\n",
    "                #Getting the token_ids of the tokens to be added\n",
    "                reshaped_top_total_token_ids=total_top_token_ids.view(original_batch_size, num_beams*num_beams)\n",
    "                \"\"\"\n",
    "                Here each row will contain the token ids of the top scoring tokens of all beams of a sequence just like the structure of reshaped_total_top_score\n",
    "                Since new_token_indices tells the row index of the topk elements of that row, the same indices will also point to the token_ids in reshaped_top_total_token_ids\n",
    "                So we make a vector of shape (original_batch_size,1) where each element will help us point to a row indices in reshaped_top_total_token_ids\n",
    "                And then new_token_indices has column indices which help us point to columns of reshaped_top_total_token_ids for that row\n",
    "                in this case of Advacne indexing because we use two rows and columns we get a new tensor of shape of rows and columns used\n",
    "                \"\"\"\n",
    "\n",
    "                raw_new_tokens = reshaped_top_total_token_ids[torch.arange(original_batch_size,device=device).unsqueeze(1), new_token_indices] #the token_ids for the new tokens in a Tensor of shape(original_batch_size, num_beams)\n",
    "                new_tokens = raw_new_tokens.view(original_batch_size*num_beams,1) #2D Tensor of shape(original_batch_size*num_beam,1) made to be suited for concatenation to decoder_input_ids\n",
    "                decoder_input_ids=torch.cat([prev_decoder_inputs, new_tokens], dim=1)\n",
    "\n",
    "                #Updating the beam scores to be used after this step in generation\n",
    "                beam_scores=new_top_scores.view(-1)\n",
    "\n",
    "                #Printing decoder_input_ids to see whats going on\n",
    "                #print(f\"Decoder input ids at step{step}:\\n{decoder_input_ids}\")\n",
    "\n",
    "                # eos_mask = (new_tokens == tokenizer.eos_token_id) #creating 2D boolean of shape as new_tokens telling which tokens generated now are EOS by setting that value to True\n",
    "                # finished_beams = finished_beams | eos_mask.view(-1) #.view(-1)Flattens eos_mask to make it a 1D tensor of shape(original_batch_size*num_beams) same as finished_beams, then we perform an OR operation(|) to update finished beam to True for the beams that have just generated EOS tokens\n",
    "\n",
    "                # Check for early stopping\n",
    "                if early_stopping:\n",
    "                    # If all beams for all sequences are finished, stop decoding\n",
    "                    if new_tokens[0]==tokenizer.eos_token_id: #if the top beam at the time generates the EOS token then break the loop\n",
    "                        break\n",
    "\n",
    "\n",
    "    # Reshape and extract the best sequence for each input\n",
    "    output_sequences = decoder_input_ids.view(original_batch_size, num_beams, -1)\n",
    "    best_sequences = output_sequences[:, 0, :] #creates 2D tensor of shape(original_batch_size, sequence length)\n",
    "\n",
    "    return best_sequences\n",
    "\n",
    "\n",
    "# Requires a translated list(list containing MT hypothesis) of previous iteration and index of current iteration\n",
    "def constrained_translate(translated_list):\n",
    "    #Tokenizing translated list\n",
    "\n",
    "    translated_list_t = tokenizer(\n",
    "                translated_list,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "    translated_input_ids=translated_list_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "    translated_attention_mask=translated_list_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "\n",
    "    dataset_t = tokenizer(\n",
    "                source_reference_list,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "    source_input_ids=dataset_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "    source_attention_mask=dataset_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "\n",
    "\n",
    "    #Tokenizing target(russian) reference list\n",
    "    reference_list_t = tokenizer(\n",
    "            target_reference_list,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            ).to(device)\n",
    "\n",
    "    reference_input_ids = reference_list_t[\"input_ids\"]  # This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "    reference_attention_mask = reference_list_t[\"attention_mask\"]  # This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "\n",
    "    constrained_translated_list=[]\n",
    "\n",
    "\n",
    "    # Iterating through each translated sequence\n",
    "    for i in tqdm(range(len(translated_list))):\n",
    "        translated_seq_ids = translated_input_ids[i:i+1]  # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Single sequence from translated batch\n",
    "        reference_seq_ids = reference_input_ids[i:i+1]  # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Corresponding reference sequence\n",
    "        source_seq_ids= source_input_ids[i:i+1] # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Corresponding reference sequence\n",
    "        source_seq_attention_mask=source_attention_mask[i:i+1]\n",
    "\n",
    "\n",
    "        # Find phrases(subsequences) missing from translated_seq to add them as constraints\n",
    "        constraint, position = find_missing_tokens(translated_seq_ids[0], reference_seq_ids[0])#constraint will be a 1D array of token_ids, Sending in values of the one row in translated_seq_ids and reference_seq_ids in find_missing_tokens, so these are 1D tensors(imagine like a list of integers)\n",
    "                                                                                    #it could also return None in case of no mismatch\n",
    "\n",
    "        #In case there is no mismatch in translated and reference list just add the sequence from translated_list in the constrained_translated_list, and continue to next sequence\n",
    "        if constraint==None:\n",
    "            constrained_translated_list.append(translated_list[i]) #.append will add the string as a single element to constrained_translated_list, if we use .extend here it will treat the string as an iterable and add each of its elements(characters) to the constrained_translated_list\n",
    "            continue\n",
    "\n",
    "        constraint_info={\"token_ids\":constraint, \"position\":position}\n",
    "        #Add the constraint to the list of constraints for the current sequence\n",
    "        GBS_constraints_3[i].append(constraint_info)\n",
    "\n",
    "\n",
    "        #Generated translation using Grid_beam_search\n",
    "        constrained_seq_ids = constrained_beam_search(\n",
    "            input_ids=source_seq_ids,#sending input_ids of one sequence\n",
    "            attention_mask=source_seq_attention_mask, #sending attention mask of 1 sequence\n",
    "            constraints=GBS_constraints_3[i], #Sending constraints for that one sequence\n",
    "        )\n",
    "\n",
    "\n",
    "        constrained_translation = tokenizer.batch_decode(constrained_seq_ids, skip_special_tokens=True)\n",
    "        constrained_translated_list.extend(constrained_translation) #we use .extend here because it will add each element of the iterable constrained_translation to constrained_translated_list, constrained_translation could be a list of strings, but in this case is just a list containing one string\n",
    "\n",
    "    return constrained_translated_list\n",
    "\n",
    "\n",
    "#Getting the translation of input sequence by deriving constraints for each sequence of target_reference_list subsequences that are missing from model_generated_list\n",
    "constrained_translation_3=constrained_translate(constrained_translation_2)  #list of Constrained Translated Sequnces in Target Language\n",
    "\n",
    "\n",
    "#BLEU score for Third iteration\n",
    "Base_blue_score= sacrebleu.corpus_bleu(model_generated_list, reference_list_BLEU)\n",
    "print(f\"\\nBase Bleu score:{Base_blue_score} \")\n",
    "#Confirm number of elements in hypotheses list is equal to number of sequences in reference list)\n",
    "if len(constrained_translation_3)==len(reference_list_BLEU[0]):\n",
    "    constrained_bleu_score3= sacrebleu.corpus_bleu(constrained_translation_3, reference_list_BLEU)\n",
    "    print(f\"\\n Constrained Bleu Score for 3rd iteration:{constrained_bleu_score3}\")\n",
    "\n",
    "else:\n",
    "    print(\"Mismatch in number of hypotheses and reference sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1352827,
     "status": "ok",
     "timestamp": 1733482620939,
     "user": {
      "displayName": "Anupreet Singh Sidhu",
      "userId": "10595161356432160963"
     },
     "user_tz": 300
    },
    "id": "jGylFqE4bAKr",
    "outputId": "a76ed443-5b32-42e7-bb34-8e5270334ac3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2818/2818 [22:28<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base Bleu score:BLEU = 27.48 57.4/33.3/21.3/14.0 (BP = 1.000 ratio = 1.012 hyp_len = 56568 ref_len = 55920) \n",
      "\n",
      " Constrained Bleu Score:BLEU = 26.32 57.1/32.2/20.2/12.9 (BP = 1.000 ratio = 1.011 hyp_len = 56549 ref_len = 55920)\n"
     ]
    }
   ],
   "source": [
    "## Implementing Lexical Constraints (Fourth iteration)\n",
    "eval_dataset=dataset[\"validation\"]\n",
    "source_reference_list=[row[\"en\"] for row in eval_dataset[\"translation\"]] #A list of all english sentences in the dataset\n",
    "target_reference_list=[row[\"ru\"] for row in eval_dataset[\"translation\"]] #A list of all russian sentence in the dataset\n",
    "reference_list_BLEU=[[row[\"ru\"] for row in eval_dataset[\"translation\"]]]\n",
    "generation_batch_size=1 #increase or decrease as per memory available in models processing unit\n",
    "model.eval()# Putting the model in eval mode\n",
    "GBS_constraints_4=GBS_constraints_3\n",
    "\"\"\"\n",
    "find_missing_tokens function returns the n-gram(subsequence) from reference_seq which is missing from translated_sequence,\n",
    "It looks for 3-grams, if none found then for 2-grams, if none found then 1-grams, and return None if not even 1-grams are found to be missing from translated_sequence\n",
    "\"\"\"\n",
    "def find_missing_tokens(translated_seq, reference_seq):\n",
    "    #This nested function returns a boolean value after checking if the subsequence is missing from the translated_seq\n",
    "    def is_subsequence_missing(subsequence, translated_seq, n):\n",
    "        #Goes over all n-grams(subsequences) in the translated_seq\n",
    "        for i in range(len(translated_seq) - n + 1):\n",
    "            #compares all n-grams in translated_seq and\n",
    "            if torch.equal(translated_seq[i:i + n], subsequence): #if one of the subsequences in translated_seq is equal to subsequence it returns false because it means subsequence is not missing from translated_seq\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Check for missing subsequences of in the order 3, 2, and 1 tokens\n",
    "    for n in range(3, 0, -1):\n",
    "        #Goes over all n-grams(subsequences) in the reference_seq\n",
    "        for i in range(len(reference_seq) - n + 1): #number of n-grams in a sequence=len_of_seq-n+1, here len(reference_seq) give the length of the reference sequence since it is a 1D tensor\n",
    "            subsequence = reference_seq[i:i + n] #choosing the current n-gram\n",
    "            if is_subsequence_missing(subsequence, translated_seq, n): #returns True if current n-gram is missing\n",
    "                return subsequence, i #basically returns the first missing n-gram we can find in the order of looking for 3,2,1-grams and the position of the first token of the n-gram in reference\n",
    "\n",
    "    # If no missing subsequences are found, return None\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def constrained_beam_search(input_ids, attention_mask, constraints, num_beams=4, max_length=128, early_stopping=True):\n",
    "    original_batch_size = input_ids.shape[0] #will be 1, because input_ids here is a 1D tensor in case we are only sending in 1 sequence from constrained_translate() to be translated in constrained_beam_search_ at a time.\n",
    "\n",
    "    #scaling original input to num_beams for beam search\n",
    "    input_ids = input_ids.repeat_interleave(num_beams, dim=0) #creating num_beam copies the each input sequence(one for each beam).\n",
    "    attention_mask = attention_mask.repeat_interleave(num_beams, dim=0) #doing the same thing again but for attention masks\n",
    "\n",
    "    # Creating tensor for storing outputs of current beams\n",
    "    decoder_input_ids = torch.full(  #making a 2D tensor of shape (original_batch_size * num_beams, 1) with each element equal to \"BOS\" token_id of the decoder/tokenizer\n",
    "                        (original_batch_size * num_beams, 1),\n",
    "                        model.config.decoder_start_token_id,\n",
    "                        dtype=torch.long,\n",
    "                        device=device\n",
    "                        )\n",
    "\n",
    "    # making a 1D tensor for storing scores of all beams of all sequences\n",
    "    beam_scores = torch.zeros(original_batch_size * num_beams, dtype=torch.float, device=device)\n",
    "\n",
    "    #Maintains a list of states for each beam\n",
    "    constraint_states = [[{\"active\": True, \"satisfied\": False, \"tokens_left\": constraint_info[\"token_ids\"].clone(), \"position\": constraint_info[\"position\"]} for constraint_info in constraints] for _ in range(original_batch_size * num_beams)] #list for each beam of all input sequences, each beam(row) has a list of constraints states\n",
    "                                                                                                                             #each constraint state is a dictionary with 3 keys- \"active\"(meaning it is actively looking to be fullfilled), \"satisfied\"(meaning it is satisfied), \"tokens_left\"(which contain a copy of the tokens to be included for that constraint in that beam)\n",
    "    Start=True ##initial Condition to be checked only once during the first forward pass                                                                                                                           #Active=True means the full constraint is pending or part of the constraint is pending\n",
    "    #Helper function for adding constraint token\n",
    "    def find_variable_in_constraints(step, constraints):\n",
    "        position_list=[]\n",
    "        for i, constraint_dict in enumerate(constraints):\n",
    "            if step == constraint_dict[\"position\"] and constraint_dict[\"active\"]==True: #Constraint must be active constraint\n",
    "                position_list.append(i)  # Return the index of the dictionary whose constraint position is equal to current decoding step\n",
    "        if position_list: #if their is anything in the position list\n",
    "            return position_list\n",
    "        return None     #Else return None\n",
    "    #Decoding Steps for generating the output\n",
    "    for step in range(max_length):#each decoding step, adding one token to a beam of decoder_input_ids\n",
    "\n",
    "        #Adding Constraint\n",
    "        constraint_num=find_variable_in_constraints(step, constraint_states[0]) #checking for position in first beam since all beams are updated together anyways, it will return constraints whose \"position\" is equal to the current step\n",
    "        if constraint_num is not None: #this step is equal to position of some constraint\n",
    "            #add constraint token to decoder_input_id\n",
    "            constraint_token_tensor=torch.full((original_batch_size * num_beams, 1), constraint_states[0][constraint_num[0]][\"tokens_left\"][0],dtype=torch.long, device=device)\n",
    "            \"\"\"\n",
    "            Explaination of above step:\n",
    "            constraint_states[0] gives us a list of dictionaries containing constraint info for all constraints of the first beam(since all constraints for all beams are updated together we can take any one, it doesnt matter)\n",
    "            then from the constraint_num list that has a list of indices for all constraints that have first token valid for this step, we choose the first one i.e.constraint_num[0](it wouldnt matter if you choose the second one too since the token for a specific position would be the same as they are all derived from the same reference translation)\n",
    "            Then we take the first token of that constraint and prepare a tensor to add it to the decoder input_ids\n",
    "            \"\"\"\n",
    "            #add the 1 token of constraint to beams of decoder_input_id\n",
    "            decoder_input_ids=torch.cat([decoder_input_ids, constraint_token_tensor], dim=1)\n",
    "\n",
    "\n",
    "            #update constraint_state for beams of the sequence\n",
    "            for i in range(original_batch_size*num_beams): #for all beams\n",
    "                for x in range(len(constraint_num)):  #for all constraints with position same as this step\n",
    "                    constraint_states[i][constraint_num[x]][\"position\"]+=1 #shift position 1 index forward for the next token\n",
    "                    constraint_states[i][constraint_num[x]][\"tokens_left\"]=constraint_states[i][constraint_num[x]][\"tokens_left\"][1:] #remove the token from the constraint that has been added to the decoder_input_id\n",
    "                    if len(constraint_states[i][constraint_num[x]][\"tokens_left\"])==0: #when there are no more tokens left in that constraint\n",
    "                        constraint_states[i][constraint_num[x]][\"active\"]=False #change the state of the constraint\n",
    "\n",
    "\n",
    "\n",
    "        #Generating token by use of logits and Scratch beam search\n",
    "        else: #this step is not a position of some constraint then we just generate using models logits\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=decoder_input_ids\n",
    "                    ) #This will be a dictionary like object containing a lot of things like hidden states,etc, but we are most interested in logits\n",
    "                \"\"\"\n",
    "                Talking about some important Tensor Operations:\n",
    "\n",
    "                Broadcasting:\n",
    "                In pytorch, a process called broadcasting happens just before an element wise operation that requires two tensors with same number of dimensions to align in shape.\n",
    "                Example Adding tensor A of shape(4,6,8) with tensor B of shape(4,1,1) then the scaler values in tensor B along the dimensions with only 1 element are duplicated to match the shape of the tensor A.\n",
    "                But broadcasting only happens when two tensors have same size of some dimensions and some dimensions of size 1.\n",
    "\n",
    "                tensor.view(size of dim=0, size of dim=1.....) is used to create a reshaped tensor with the same number of elements as the original tensor.\n",
    "                It can add new dimensions of a specific size as long as the number of element in the original tensor could be accomodated.\n",
    "                tensor.view(-1,10) tell us to accomodate 10 elements in dim=10 and infer what number of element to accomodate in dim=0 such that the total number of elements remain the same.\n",
    "\n",
    "                tensor.unsqueeze() is used to add a singleton dimension(dimension of size 1) at a specified index\n",
    "                tensor.unsequeeze(1) adds a singleton dimension at index=1\n",
    "\n",
    "                tensor.squeeze() is used remove a singleton dimension from a specified index\n",
    "\n",
    "                tensor.expand() is used to basically perform broadcasting on command, you would send in sizes of dimension of tensor which are capable of broadcasting and get a result.\n",
    "                This means that the size you send in should either be the same as sizes of dimension of original tensor OR in case of singleton dimension in original tensor you could send in a greater number to stretch along that dimension\n",
    "                tensor.expand(-1, 4) tells to keep dim=0 as is and basically stretch dim=1 to size 4, this is conditioned on the fact that the dimension to be stretched is of size 1 otherwise an error will occur\n",
    "\n",
    "                tensor.expand_as(target_tensor) is used to perform broadcasting on tensor conditioned on the fact that the size of target_tensor is suitable for broadcasting\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                # Creating a 2D tensor Logits\n",
    "                logits = outputs.logits[:, -1, :] #output.logits is a 3-D tensor of shape(original_batch_size*num_beam, current sequence length in decoder_input_ids, vocab size). where each element is the logit of each token_id of each sequence for each element of the vocabulary\n",
    "                                                #By slicing we choose the last token_id of each sequence it to get a 2-D tensor of shape(original_batch_size*num_beam, vocab_size) where each element is a logit of last token_id of each sequence element for each element of the vocabulary\n",
    "                #print(f\"Logits at step {step}:\\n{logits}\")\n",
    "                # Using Boolean masking to mark finished beams (those that already generated EOS)\n",
    "                #finished_mask = finished_beams.view(-1, 1).expand(-1, logits.size(-1))  # finished_mask will be a 2D of shape(original_batch_size * num_beams, vocab_size) with values of True/False repeated in each row(representing each beam) upto the vocab_size\n",
    "                #logits[finished_mask] = -float('inf')  # Making logits of lasts token for all elements of vocabulary for the finished beams negative infinity, By sending in boolean mask tensor of the same shape as the logits tensor the operation is applied to all logits elements that correspond with True in finished mask.\n",
    "                                                        #Making Logit for all words in vocabulary negative infinity means probability=0 after softmax, and log of that is a big negative value, when that big negative value is combined with beam_score to get next_score it ensures that these beams will now not come in the topk.\n",
    "                                                        #This ensures that only active beams (those not yet finished) are considered during the top-k selection.\n",
    "\n",
    "                # Creating 2D tensor log probabilities\n",
    "                log_probs = F.log_softmax(logits, dim=-1) #performs softmax on all logits normalizing them to probability of 0-1, then takes log of that so the highest probability is turend into the largest number(smallest negative value).\n",
    "                                                        #dim=-1 takes softmax along the last dimension(2nd dimension with size=vocab_size in this case) which means that normalization happens for each row(last token for each sequence of decoder_input_ids)\n",
    "                #print(f\"Log_probs at step {step}:\\n{log_probs}\")\n",
    "                #get top num_beams scores from each beam\n",
    "                total_top_log_probs, total_top_token_ids=torch.topk(log_probs, num_beams, dim=1) #give 2 tensors of shape(original_batch_size*num_beams, num_beams) where each row contains scores top num_beam scores for that row and the corresponding token_ids in descending\n",
    "                #print(f\"Topk log_probs at step {step}:\\n{total_top_log_probs}\")\n",
    "                #print(f\"Topk token_ids at step {step}:\\n{total_top_token_ids}\")\n",
    "\n",
    "                if Start==True:\n",
    "                    new_tokens=total_top_token_ids[::num_beams].reshape(-1,1) #using slicing with step value num_beam we take the top 4 tokens of a beam of a sequence and make a tensor containing new tokens for each beam, .reshape is used in place of .view() becauce we used slicing and so tensor is not contiguous in memory hence .view wont work\n",
    "                    decoder_input_ids=torch.cat([decoder_input_ids[torch.arange(original_batch_size*num_beams)],new_tokens], dim=1) #adding first 4 tokens to different beam of each sequence\n",
    "                    beam_scores=total_top_log_probs[::num_beams].reshape(-1) #slicing to make tensor of shape(original_batch_size,num_beams) and then we flatten it using .reshape(-1)\n",
    "                    Start=False\n",
    "                    #print(f\"Decoder_input_ids at step{step}:\\n{decoder_input_ids}\")\n",
    "                    #print(f\"Beam score at step{step}:\\n{beam_scores}\")\n",
    "\n",
    "                    continue\n",
    "\n",
    "                #calculate total scores for each beams top 4 tokens\n",
    "                total_top_score=total_top_log_probs+beam_scores.unsqueeze(1)\n",
    "                #print(f\"Total score of each topk token of each beam scores at step {step}:\\n{total_top_score}\")\n",
    "\n",
    "\n",
    "                #reshape that\n",
    "                reshaped_total_top_score=total_top_score.view(original_batch_size, num_beams*num_beams)\n",
    "\n",
    "                #Getting the best scores for this step in generation\n",
    "                new_top_scores,new_token_indices=torch.topk(reshaped_total_top_score,num_beams, dim=1) #gives 2 tensors of shape (original_batch_size, num_beam) containing the top num_beam score from all total_scores of one sequence in each row in descending order and their index in that row in total_top_score_reshaped\n",
    "                #print(f\"Total Top scores for new tokens of each sequence scores at step {step}:\\n{new_top_scores}\")\n",
    "                #calculating beam indices for the new tokens of each sequence with best score\n",
    "                beam_indices=new_token_indices//num_beams #elements of a row in this tensor will tell which token originally belonged to which beam for a sequence\n",
    "                #reshaping beam_indices and scaling indices of elements from row level to global level in the tensor\n",
    "                base_indices=torch.arange(original_batch_size,device=device).unsqueeze(1) * num_beams\n",
    "                scaled_beam_indices=beam_indices+base_indices #Tensor of shape shape (original_batch_size, num_beam)\n",
    "                flattened_scaled_beam_indices=scaled_beam_indices.view(-1) #making a 1D tensor of shape(original_batch_size*num_beam)\n",
    "\n",
    "                #By Advance Indexing using these scaled indices we will obtain sequences from decoder_input_ids of the respective beam that the indices point to\n",
    "                prev_decoder_inputs=decoder_input_ids[flattened_scaled_beam_indices]\n",
    "\n",
    "                #Getting the token_ids of the tokens to be added\n",
    "                reshaped_top_total_token_ids=total_top_token_ids.view(original_batch_size, num_beams*num_beams)\n",
    "                \"\"\"\n",
    "                Here each row will contain the token ids of the top scoring tokens of all beams of a sequence just like the structure of reshaped_total_top_score\n",
    "                Since new_token_indices tells the row index of the topk elements of that row, the same indices will also point to the token_ids in reshaped_top_total_token_ids\n",
    "                So we make a vector of shape (original_batch_size,1) where each element will help us point to a row indices in reshaped_top_total_token_ids\n",
    "                And then new_token_indices has column indices which help us point to columns of reshaped_top_total_token_ids for that row\n",
    "                in this case of Advacne indexing because we use two rows and columns we get a new tensor of shape of rows and columns used\n",
    "                \"\"\"\n",
    "\n",
    "                raw_new_tokens = reshaped_top_total_token_ids[torch.arange(original_batch_size,device=device).unsqueeze(1), new_token_indices] #the token_ids for the new tokens in a Tensor of shape(original_batch_size, num_beams)\n",
    "                new_tokens = raw_new_tokens.view(original_batch_size*num_beams,1) #2D Tensor of shape(original_batch_size*num_beam,1) made to be suited for concatenation to decoder_input_ids\n",
    "                decoder_input_ids=torch.cat([prev_decoder_inputs, new_tokens], dim=1)\n",
    "\n",
    "                #Updating the beam scores to be used after this step in generation\n",
    "                beam_scores=new_top_scores.view(-1)\n",
    "\n",
    "                #Printing decoder_input_ids to see whats going on\n",
    "                #print(f\"Decoder input ids at step{step}:\\n{decoder_input_ids}\")\n",
    "\n",
    "                # eos_mask = (new_tokens == tokenizer.eos_token_id) #creating 2D boolean of shape as new_tokens telling which tokens generated now are EOS by setting that value to True\n",
    "                # finished_beams = finished_beams | eos_mask.view(-1) #.view(-1)Flattens eos_mask to make it a 1D tensor of shape(original_batch_size*num_beams) same as finished_beams, then we perform an OR operation(|) to update finished beam to True for the beams that have just generated EOS tokens\n",
    "\n",
    "                # Check for early stopping\n",
    "                if early_stopping:\n",
    "                    # If all beams for all sequences are finished, stop decoding\n",
    "                    if new_tokens[0]==tokenizer.eos_token_id: #if the top beam at the time generates the EOS token then break the loop\n",
    "                        break\n",
    "\n",
    "\n",
    "    # Reshape and extract the best sequence for each input\n",
    "    output_sequences = decoder_input_ids.view(original_batch_size, num_beams, -1)\n",
    "    best_sequences = output_sequences[:, 0, :] #creates 2D tensor of shape(original_batch_size, sequence length)\n",
    "\n",
    "    return best_sequences\n",
    "\n",
    "\n",
    "# Requires a translated list(list containing MT hypothesis) of previous iteration and index of current iteration\n",
    "def constrained_translate(translated_list):\n",
    "    #Tokenizing translated list\n",
    "\n",
    "    translated_list_t = tokenizer(\n",
    "                translated_list,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "    translated_input_ids=translated_list_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "    translated_attention_mask=translated_list_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "\n",
    "    dataset_t = tokenizer(\n",
    "                source_reference_list,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "    source_input_ids=dataset_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "    source_attention_mask=dataset_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "\n",
    "\n",
    "    #Tokenizing target(russian) reference list\n",
    "    reference_list_t = tokenizer(\n",
    "            target_reference_list,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            ).to(device)\n",
    "\n",
    "    reference_input_ids = reference_list_t[\"input_ids\"]  # This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "    reference_attention_mask = reference_list_t[\"attention_mask\"]  # This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "\n",
    "    constrained_translated_list=[]\n",
    "\n",
    "\n",
    "    # Iterating through each translated sequence\n",
    "    for i in tqdm(range(len(translated_list))):\n",
    "        translated_seq_ids = translated_input_ids[i:i+1]  # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Single sequence from translated batch\n",
    "        reference_seq_ids = reference_input_ids[i:i+1]  # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Corresponding reference sequence\n",
    "        source_seq_ids= source_input_ids[i:i+1] # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Corresponding reference sequence\n",
    "        source_seq_attention_mask=source_attention_mask[i:i+1]\n",
    "\n",
    "\n",
    "        # Find phrases(subsequences) missing from translated_seq to add them as constraints\n",
    "        constraint, position = find_missing_tokens(translated_seq_ids[0], reference_seq_ids[0])#constraint will be a 1D array of token_ids, Sending in values of the one row in translated_seq_ids and reference_seq_ids in find_missing_tokens, so these are 1D tensors(imagine like a list of integers)\n",
    "                                                                                    #it could also return None in case of no mismatch\n",
    "\n",
    "        #In case there is no mismatch in translated and reference list just add the sequence from translated_list in the constrained_translated_list, and continue to next sequence\n",
    "        if constraint==None:\n",
    "            constrained_translated_list.append(translated_list[i]) #.append will add the string as a single element to constrained_translated_list, if we use .extend here it will treat the string as an iterable and add each of its elements(characters) to the constrained_translated_list\n",
    "            continue\n",
    "\n",
    "        constraint_info={\"token_ids\":constraint, \"position\":position}\n",
    "        #Add the constraint to the list of constraints for the current sequence\n",
    "        GBS_constraints_4[i].append(constraint_info)\n",
    "\n",
    "\n",
    "        #Generated translation using Grid_beam_search\n",
    "        constrained_seq_ids = constrained_beam_search(\n",
    "            input_ids=source_seq_ids,#sending input_ids of one sequence\n",
    "            attention_mask=source_seq_attention_mask, #sending attention mask of 1 sequence\n",
    "            constraints=GBS_constraints_4[i], #Sending constraints for that one sequence\n",
    "        )\n",
    "\n",
    "\n",
    "        constrained_translation = tokenizer.batch_decode(constrained_seq_ids, skip_special_tokens=True)\n",
    "        constrained_translated_list.extend(constrained_translation) #we use .extend here because it will add each element of the iterable constrained_translation to constrained_translated_list, constrained_translation could be a list of strings, but in this case is just a list containing one string\n",
    "\n",
    "    return constrained_translated_list\n",
    "\n",
    "\n",
    "#Getting the translation of input sequence by deriving constraints for each sequence of target_reference_list subsequences that are missing from model_generated_list\n",
    "constrained_translation_4=constrained_translate(constrained_translation_3)  #list of Constrained Translated Sequnces in Target Language\n",
    "\n",
    "\n",
    "#BLEU score for Third iteration\n",
    "Base_blue_score= sacrebleu.corpus_bleu(model_generated_list, reference_list_BLEU)\n",
    "print(f\"\\nBase Bleu score:{Base_blue_score} \")\n",
    "#Confirm number of elements in hypotheses list is equal to number of sequences in reference list)\n",
    "if len(constrained_translation_4)==len(reference_list_BLEU[0]):\n",
    "    constrained_bleu_score4= sacrebleu.corpus_bleu(constrained_translation_4, reference_list_BLEU)\n",
    "    print(f\"\\n Constrained Bleu Score for 4rth iteration:{constrained_bleu_score4}\")\n",
    "\n",
    "else:\n",
    "    print(\"Mismatch in number of hypotheses and reference sequences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Trying Constrained Beam Search for single instances in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developer Interpretation of what is happening here:\n",
    "1. We tokenize an input English sentence then use forward pass and generate condensed decoder_input_ids. These input ids could be decoded and formed a sentence. These sentences are then added to a translated sequence list and used as baseline for implementing constraints for the first time.\n",
    "2. But when we tokenize that sentence using the tokenizer(setting the max_length=128) it expands and maybe even overflow thus being truncated and losing tokens at the end. Then if you try to decode that sentence you will see loss of words at the end. \n",
    "3. That said there are still many constraints that could be even if some sentences overflow and get truncated because others wont overflow and retain all information and the ones that do overflow would probably still have a constraints to be identified by comparing them to reference translations. \n",
    "4. Now we implement constraints by observing the tokenized forms of reference translations and try to identify sub-sequences up-to 3 tokens for each sequence in them, that are not present in baseline translation. \n",
    "5. For first iteration/cycle of applying constraints we choose 1 constraint for each sequence and get translation of input sequences using constrained_beam_search(). Now that we have these translations from the first translation we store them in a list.\n",
    "6. For our second iteration we identify sub-sequences of up to 3 tokens in the tokenized form of reference translation that are missing from the tokenized form of sequences of translations from first iteration, these sub-sequences are our second set of constraints. We use the both set of constraints to get translation of input sequences using the constrained_beam_search(). \n",
    "7. For all subsequent iterations we could keep identifying additional sets of constraints by using reference translations and the translations of the previous iteration. \n",
    "8. As we keep applying constraints our translations naturally become more and more similar to reference translation so our BLEU score increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Points to keep in mind when running this: \n",
    "* Obtaining the model_generated_list(the baseline translation list) will be a prerequisite for implementing constraints in this first iteration. Make sure you have that part correct before making observations\n",
    "* Then just change the splicing indices at the top of this cell to any value of one element.  \n",
    "* For seeing constraints of subsequent iteration just update the GBS_constraint variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "23I85l9JMcLc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reference sequence Token id:\n",
      "tensor([  21, 1903,   41,  222,  779,   70,  222,   38,   21, 4113,   70,  230,\n",
      "         230,   53,  229,   21,  141,   30,   21,  230,  275,   41,  141,   30,\n",
      "         275,   71,   44,  374,   70,  971,   41,  275,  426,   21,  260,  222,\n",
      "          30,  779,  426,  173,  198,   21,  222,   21,  230,  222,  229,  670,\n",
      "          53,   21,  230,   21,  141,   70,  222,  100,   46,   53,   21,  230,\n",
      "          41,  141,  557, 3699,   53,  229,   46,   53,   21,   53,   21,  141,\n",
      "          30,   21, 1209,   60,  260,   30,  275,   21,  222,  100,   44,   70,\n",
      "         260,   53,  275,  426,   21,   53,  670,   21,  746, 1676, 1755,    0])\n",
      "The Hypotheses sequence Token id:\n",
      "tensor([  21, 4113,   70,  230,  230,   53,  229,   21,  141,   30,   21,  670,\n",
      "          41,   44,  374,   70,  971,  141,   30,  275,   21,  260,  222,   30,\n",
      "         779,  426,   21,  971,   30,  779,   30,  260,   21,  374,   53, 3699,\n",
      "          70,   46,   21,  141,   70,  222,  100,   44,   21,  230,   41,  141,\n",
      "         557, 3699,   53,   48,    3,    0])\n",
      "constraint Identified in this iteration at position 0:\n",
      "tensor([  21, 1903,   41])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids of the Constrained_Beam_Search output which will be decoded and stored as our constrained translation:\n",
      "tensor([[62517,    21,  1903,    41,   222,  4053,    38, 23453,    26,    52,\n",
      "            44, 20391,  3573,  8471,   916,  7014,  1149,  6170,     7,    26,\n",
      "            15,   451,  3573,  8271,     0]])\n",
      "The token_ids after tokenizing the constrained translation showing the constrained token_ids are still applied:\n",
      "tensor([[  21, 1903,   41,  222,  779,   70,  222,   38,   21, 4113,   70,  230,\n",
      "          230,   53,  229,   21,  141,   30,   21,  670,   41,   44,  374,   70,\n",
      "          971,  141,   30,  275,   21,  260,  222,   30,  779,  426,   21,  971,\n",
      "           30,  779,   30,  260,   21,  374,   53, 3699,   70,   46,   21,  141,\n",
      "           70,  222,  100,   44,   21,  230,   41,  141,  557, 3699,   53,   48,\n",
      "           21,   53,   21,  141,   30,   21,  971,   70,  557,   53,  141,   30,\n",
      "          275,   21,  746, 1676, 1755,    0]])\n",
      "\n",
      "Base Bleu score:BLEU = 2.81 22.2/12.5/7.1/4.2 (BP = 0.295 ratio = 0.450 hyp_len = 9 ref_len = 20) \n",
      "\n",
      " BLEU score for the constrained translation Sequence:BLEU = 14.31 50.0/30.8/16.7/9.1 (BP = 0.651 ratio = 0.700 hyp_len = 14 ref_len = 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Checking and confirming applied constraints one for single seqeunce\n",
    "\n",
    "eval_dataset=dataset[\"validation\"]\n",
    "source_reference_list=[row[\"en\"] for row in eval_dataset[\"translation\"]][54:55] #A list of all english sentences in the dataset\n",
    "target_reference_list=[row[\"ru\"] for row in eval_dataset[\"translation\"]][54:55] #A list of all russian sentence in the dataset\n",
    "reference_list_BLEU=[[row[\"ru\"] for row in eval_dataset[\"translation\"][54:55]]]\n",
    "generation_batch_size=1 #increase or decrease as per memory available in models processing unit\n",
    "model.eval()# Putting the model in eval mode\n",
    "GBS_constraints=[[] for _ in range(len(source_reference_list))] # we make a list containing len(source_reference_list) empty lists where each empty list will contain contraints for each sequence in the future This will be list of lists that contain subsequences(lexical constraints) for each input sequence in eval_dataset,\n",
    "\n",
    "\"\"\"\n",
    "find_missing_tokens function returns the n-gram(subsequence) from reference_seq which is missing from translated_sequence,\n",
    "It looks for 3-grams, if none found then for 2-grams, if none found then 1-grams, and return None if not even 1-grams are found to be missing from translated_sequence\n",
    "\"\"\"\n",
    "def find_missing_tokens(translated_seq, reference_seq):\n",
    "    #This nested function returns a boolean value after checking if the subsequence is missing from the translated_seq\n",
    "    def is_subsequence_missing(subsequence, translated_seq, n):\n",
    "        #Goes over all n-grams(subsequences) in the translated_seq\n",
    "        for i in range(len(translated_seq) - n + 1):\n",
    "            #compares all n-grams in translated_seq and\n",
    "            if torch.equal(translated_seq[i:i + n], subsequence): #if one of the subsequences in translated_seq is equal to subsequence it returns false because it means subsequence is not missing from translated_seq\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    # Check for missing subsequences of in the order 3, 2, and 1 tokens\n",
    "    for n in range(3, 0, -1):\n",
    "        #Goes over all n-grams(subsequences) in the reference_seq\n",
    "        for i in range(len(reference_seq) - n + 1): #number of n-grams in a sequence=len_of_seq-n+1, here len(reference_seq) give the length of the reference sequence since it is a 1D tensor\n",
    "            subsequence = reference_seq[i:i + n] #choosing the current n-gram\n",
    "            if is_subsequence_missing(subsequence, translated_seq, n): #returns True if current n-gram is missing\n",
    "                return subsequence, i #basically returns the first missing n-gram we can find in the order of looking for 3,2,1-grams and the position of the first token of the n-gram in reference\n",
    "\n",
    "    # If no missing subsequences are found, return None\n",
    "    return None, None\n",
    "\n",
    "#The main function for implementing constraints\n",
    "def constrained_beam_search(input_ids, attention_mask, constraints, num_beams=4, max_length=128, early_stopping=True):\n",
    "    original_batch_size = input_ids.shape[0] #will be 1, because input_ids here is a 1D tensor in case we are only sending in 1 sequence from constrained_translate() to be translated in constrained_beam_search_ at a time.\n",
    "\n",
    "    #scaling original input to num_beams for beam search\n",
    "    input_ids = input_ids.repeat_interleave(num_beams, dim=0) #creating num_beam copies the each input sequence(one for each beam).\n",
    "    attention_mask = attention_mask.repeat_interleave(num_beams, dim=0) #doing the same thing again but for attention masks\n",
    "\n",
    "    # Creating tensor for storing outputs of current beams\n",
    "    decoder_input_ids = torch.full(  #making a 2D tensor of shape (original_batch_size * num_beams, 1) with each element equal to \"BOS\" token_id of the decoder/tokenizer\n",
    "                        (original_batch_size * num_beams, 1),\n",
    "                        model.config.decoder_start_token_id,\n",
    "                        dtype=torch.long,\n",
    "                        device=device\n",
    "                        )\n",
    "\n",
    "    # making a 1D tensor for storing scores of all beams of all sequences\n",
    "    beam_scores = torch.zeros(original_batch_size * num_beams, dtype=torch.float, device=device)\n",
    "\n",
    "    #Maintains a list of states for each beam\n",
    "    constraint_states = [[{\"active\": True, \"satisfied\": False, \"tokens_left\": constraint_info[\"token_ids\"].clone(), \"position\": constraint_info[\"position\"]} for constraint_info in constraints] for _ in range(original_batch_size * num_beams)] #list for each beam of all input sequences, each beam(row) has a list of constraints states\n",
    "                                                                                                                             #each constraint state is a dictionary with 3 keys- \"active\"(meaning it is actively looking to be fullfilled), \"satisfied\"(meaning it is satisfied), \"tokens_left\"(which contain a copy of the tokens to be included for that constraint in that beam)\n",
    "    Start=True ##initial Condition to be checked only once during the first forward pass                                                                                                                           #Active=True means the full constraint is pending or part of the constraint is pending\n",
    "    #Helper function for adding constraint token\n",
    "    def find_variable_in_constraints(step, constraints):\n",
    "        position_list=[]\n",
    "        for i, constraint_dict in enumerate(constraints):\n",
    "            if step == constraint_dict[\"position\"] and constraint_dict[\"active\"]==True: #Constraint must be active constraint\n",
    "                position_list.append(i)  # Return the index of the dictionary whose constraint position is equal to current decoding step\n",
    "        if position_list: #if their is anything in the position list\n",
    "            return position_list\n",
    "        return None     #Else return None\n",
    "    #Decoding Steps for generating the output\n",
    "    for step in range(max_length):#each decoding step, adding one token to a beam of decoder_input_ids\n",
    "\n",
    "        #Adding Constraint\n",
    "        constraint_num=find_variable_in_constraints(step, constraint_states[0]) #checking for position in first beam since all beams are updated together anyways, it will return constraints whose \"position\" is equal to the current step\n",
    "        if constraint_num is not None: #this step is equal to position of some constraint\n",
    "            #add constraint token to decoder_input_id\n",
    "            constraint_token_tensor=torch.full((original_batch_size * num_beams, 1), constraint_states[0][constraint_num[0]][\"tokens_left\"][0],dtype=torch.long, device=device)\n",
    "            \"\"\"\n",
    "            Explaination of above step:\n",
    "            constraint_states[0] gives us a list of dictionaries containing constraint info for all constraints of the first beam(since all constraints for all beams are updated together we can take any one, it doesnt matter)\n",
    "            then from the constraint_num list that has a list of indices for all constraints that have first token valid for this step, we choose the first one i.e.constraint_num[0](it wouldnt matter if you choose the second one too since the token for a specific position would be the same as they are all derived from the same reference translation)\n",
    "            Then we take the first token of that constraint and prepare a tensor to add it to the decoder input_ids\n",
    "            \"\"\"\n",
    "            #add the 1 token of constraint to beams of decoder_input_id\n",
    "            decoder_input_ids=torch.cat([decoder_input_ids, constraint_token_tensor], dim=1)\n",
    "\n",
    "\n",
    "            #update constraint_state for beams of the sequence\n",
    "            for i in range(original_batch_size*num_beams): #for all beams\n",
    "                for x in range(len(constraint_num)):  #for all constraints with position same as this step\n",
    "                    constraint_states[i][constraint_num[x]][\"position\"]+=1 #shift position 1 index forward for the next token\n",
    "                    constraint_states[i][constraint_num[x]][\"tokens_left\"]=constraint_states[i][constraint_num[x]][\"tokens_left\"][1:] #remove the token from the constraint that has been added to the decoder_input_id\n",
    "                    if len(constraint_states[i][constraint_num[x]][\"tokens_left\"])==0: #when there are no more tokens left in that constraint\n",
    "                        constraint_states[i][constraint_num[x]][\"active\"]=False #change the state of the constraint\n",
    "\n",
    "\n",
    "\n",
    "        #Generating token by use of logits and Scratch beam search\n",
    "        else: #this step is not a position of some constraint then we just generate using models logits\n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=decoder_input_ids\n",
    "                    ) #This will be a dictionary like object containing a lot of things like hidden states,etc, but we are most interested in logits\n",
    "                \"\"\"\n",
    "                Talking about some important Tensor Operations:\n",
    "\n",
    "                Broadcasting:\n",
    "                In pytorch, a process called broadcasting happens just before an element wise operation that requires two tensors with same number of dimensions to align in shape.\n",
    "                Example Adding tensor A of shape(4,6,8) with tensor B of shape(4,1,1) then the scaler values in tensor B along the dimensions with only 1 element are duplicated to match the shape of the tensor A.\n",
    "                But broadcasting only happens when two tensors have same size of some dimensions and some dimensions of size 1.\n",
    "\n",
    "                tensor.view(size of dim=0, size of dim=1.....) is used to create a reshaped tensor with the same number of elements as the original tensor.\n",
    "                It can add new dimensions of a specific size as long as the number of element in the original tensor could be accomodated.\n",
    "                tensor.view(-1,10) tell us to accomodate 10 elements in dim=10 and infer what number of element to accomodate in dim=0 such that the total number of elements remain the same.\n",
    "\n",
    "                tensor.unsqueeze() is used to add a singleton dimension(dimension of size 1) at a specified index\n",
    "                tensor.unsequeeze(1) adds a singleton dimension at index=1\n",
    "\n",
    "                tensor.squeeze() is used remove a singleton dimension from a specified index\n",
    "\n",
    "                tensor.expand() is used to basically perform broadcasting on command, you would send in sizes of dimension of tensor which are capable of broadcasting and get a result.\n",
    "                This means that the size you send in should either be the same as sizes of dimension of original tensor OR in case of singleton dimension in original tensor you could send in a greater number to stretch along that dimension\n",
    "                tensor.expand(-1, 4) tells to keep dim=0 as is and basically stretch dim=1 to size 4, this is conditioned on the fact that the dimension to be stretched is of size 1 otherwise an error will occur\n",
    "\n",
    "                tensor.expand_as(target_tensor) is used to perform broadcasting on tensor conditioned on the fact that the size of target_tensor is suitable for broadcasting\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                # Creating a 2D tensor Logits\n",
    "                logits = outputs.logits[:, -1, :] #output.logits is a 3-D tensor of shape(original_batch_size*num_beam, current sequence length in decoder_input_ids, vocab size). where each element is the logit of each token_id of each sequence for each element of the vocabulary\n",
    "                                                #By slicing we choose the last token_id of each sequence it to get a 2-D tensor of shape(original_batch_size*num_beam, vocab_size) where each element is a logit of last token_id of each sequence element for each element of the vocabulary\n",
    "                #print(f\"Logits at step {step}:\\n{logits}\")\n",
    "                # Using Boolean masking to mark finished beams (those that already generated EOS)\n",
    "                #finished_mask = finished_beams.view(-1, 1).expand(-1, logits.size(-1))  # finished_mask will be a 2D of shape(original_batch_size * num_beams, vocab_size) with values of True/False repeated in each row(representing each beam) upto the vocab_size\n",
    "                #logits[finished_mask] = -float('inf')  # Making logits of lasts token for all elements of vocabulary for the finished beams negative infinity, By sending in boolean mask tensor of the same shape as the logits tensor the operation is applied to all logits elements that correspond with True in finished mask.\n",
    "                                                        #Making Logit for all words in vocabulary negative infinity means probability=0 after softmax, and log of that is a big negative value, when that big negative value is combined with beam_score to get next_score it ensures that these beams will now not come in the topk.\n",
    "                                                        #This ensures that only active beams (those not yet finished) are considered during the top-k selection.\n",
    "\n",
    "                # Creating 2D tensor log probabilities\n",
    "                log_probs = F.log_softmax(logits, dim=-1) #performs softmax on all logits normalizing them to probability of 0-1, then takes log of that so the highest probability is turend into the largest number(smallest negative value).\n",
    "                                                        #dim=-1 takes softmax along the last dimension(2nd dimension with size=vocab_size in this case) which means that normalization happens for each row(last token for each sequence of decoder_input_ids)\n",
    "                #print(f\"Log_probs at step {step}:\\n{log_probs}\")\n",
    "                #get top num_beams scores from each beam\n",
    "                total_top_log_probs, total_top_token_ids=torch.topk(log_probs, num_beams, dim=1) #give 2 tensors of shape(original_batch_size*num_beams, num_beams) where each row contains scores top num_beam scores for that row and the corresponding token_ids in descending\n",
    "                #print(f\"Topk log_probs at step {step}:\\n{total_top_log_probs}\")\n",
    "                #print(f\"Topk token_ids at step {step}:\\n{total_top_token_ids}\")\n",
    "\n",
    "                if Start==True:\n",
    "                    new_tokens=total_top_token_ids[::num_beams].reshape(-1,1) #using slicing with step value num_beam we take the top 4 tokens of a beam of a sequence and make a tensor containing new tokens for each beam, .reshape is used in place of .view() becauce we used slicing and so tensor is not contiguous in memory hence .view wont work\n",
    "                    decoder_input_ids=torch.cat([decoder_input_ids[torch.arange(original_batch_size*num_beams)],new_tokens], dim=1) #adding first 4 tokens to different beam of each sequence\n",
    "                    beam_scores=total_top_log_probs[::num_beams].reshape(-1) #slicing to make tensor of shape(original_batch_size,num_beams) and then we flatten it using .reshape(-1)\n",
    "                    Start=False\n",
    "                    #print(f\"Decoder_input_ids at step{step}:\\n{decoder_input_ids}\")\n",
    "                    #print(f\"Beam score at step{step}:\\n{beam_scores}\")\n",
    "\n",
    "                    continue\n",
    "\n",
    "                #calculate total scores for each beams top 4 tokens\n",
    "                total_top_score=total_top_log_probs+beam_scores.unsqueeze(1)\n",
    "                #print(f\"Total score of each topk token of each beam scores at step {step}:\\n{total_top_score}\")\n",
    "\n",
    "\n",
    "                #reshape that\n",
    "                reshaped_total_top_score=total_top_score.view(original_batch_size, num_beams*num_beams)\n",
    "\n",
    "                #Getting the best scores for this step in generation\n",
    "                new_top_scores,new_token_indices=torch.topk(reshaped_total_top_score,num_beams, dim=1) #gives 2 tensors of shape (original_batch_size, num_beam) containing the top num_beam score from all total_scores of one sequence in each row in descending order and their index in that row in total_top_score_reshaped\n",
    "                #print(f\"Total Top scores for new tokens of each sequence scores at step {step}:\\n{new_top_scores}\")\n",
    "                #calculating beam indices for the new tokens of each sequence with best score\n",
    "                beam_indices=new_token_indices//num_beams #elements of a row in this tensor will tell which token originally belonged to which beam for a sequence\n",
    "                #reshaping beam_indices and scaling indices of elements from row level to global level in the tensor\n",
    "                base_indices=torch.arange(original_batch_size,device=device).unsqueeze(1) * num_beams\n",
    "                scaled_beam_indices=beam_indices+base_indices #Tensor of shape shape (original_batch_size, num_beam)\n",
    "                flattened_scaled_beam_indices=scaled_beam_indices.view(-1) #making a 1D tensor of shape(original_batch_size*num_beam)\n",
    "\n",
    "                #By Advance Indexing using these scaled indices we will obtain sequences from decoder_input_ids of the respective beam that the indices point to\n",
    "                prev_decoder_inputs=decoder_input_ids[flattened_scaled_beam_indices]\n",
    "\n",
    "                #Getting the token_ids of the tokens to be added\n",
    "                reshaped_top_total_token_ids=total_top_token_ids.view(original_batch_size, num_beams*num_beams)\n",
    "                \"\"\"\n",
    "                Here each row will contain the token ids of the top scoring tokens of all beams of a sequence just like the structure of reshaped_total_top_score\n",
    "                Since new_token_indices tells the row index of the topk elements of that row, the same indices will also point to the token_ids in reshaped_top_total_token_ids\n",
    "                So we make a vector of shape (original_batch_size,1) where each element will help us point to a row indices in reshaped_top_total_token_ids\n",
    "                And then new_token_indices has column indices which help us point to columns of reshaped_top_total_token_ids for that row\n",
    "                in this case of Advacne indexing because we use two rows and columns we get a new tensor of shape of rows and columns used\n",
    "                \"\"\"\n",
    "\n",
    "                raw_new_tokens = reshaped_top_total_token_ids[torch.arange(original_batch_size,device=device).unsqueeze(1), new_token_indices] #the token_ids for the new tokens in a Tensor of shape(original_batch_size, num_beams)\n",
    "                new_tokens = raw_new_tokens.view(original_batch_size*num_beams,1) #2D Tensor of shape(original_batch_size*num_beam,1) made to be suited for concatenation to decoder_input_ids\n",
    "                decoder_input_ids=torch.cat([prev_decoder_inputs, new_tokens], dim=1)\n",
    "\n",
    "                #Updating the beam scores to be used after this step in generation\n",
    "                beam_scores=new_top_scores.view(-1)\n",
    "\n",
    "                #Printing decoder_input_ids to see whats going on\n",
    "                #print(f\"Decoder input ids at step{step}:\\n{decoder_input_ids}\")\n",
    "\n",
    "                # Check for early stopping\n",
    "                if early_stopping:\n",
    "                    # If all beams for all sequences are finished, stop decoding\n",
    "                    if new_tokens[0]==tokenizer.eos_token_id: #if the top beam at the time generates the EOS token then break the loop\n",
    "                        break\n",
    "\n",
    "\n",
    "    # Reshape and extract the best sequence for each input\n",
    "    output_sequences = decoder_input_ids.view(original_batch_size, num_beams, -1)\n",
    "    best_sequences = output_sequences[:, 0, :] #creates 2D tensor of shape(original_batch_size, sequence length)\n",
    "\n",
    "    return best_sequences\n",
    "\n",
    "\n",
    "# Requires a translated list(list containing MT hypothesis) of previous iteration and index of current iteration\n",
    "def constrained_translate(translated_list):\n",
    "    #Tokenizing translated list\n",
    "\n",
    "    translated_list_t = tokenizer(\n",
    "                translated_list,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "    translated_input_ids=translated_list_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "    translated_attention_mask=translated_list_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "\n",
    "    dataset_t = tokenizer(\n",
    "                source_reference_list,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "    source_input_ids=dataset_t[\"input_ids\"] #This will be a 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "    source_attention_mask=dataset_t[\"attention_mask\"] #This will be 2D tensor of shape(total number of sequences tokenized, i.e. length of list passed in tokenizer, sequence length)\n",
    "\n",
    "\n",
    "    #Tokenizing target(russian) reference list\n",
    "    reference_list_t = tokenizer(\n",
    "            target_reference_list,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            ).to(device)\n",
    "\n",
    "    reference_input_ids = reference_list_t[\"input_ids\"]  # This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "    reference_attention_mask = reference_list_t[\"attention_mask\"]  # This will be 2D tensor of shape(total number of sequences tokenized, sequence length)\n",
    "\n",
    "    constrained_translated_list=[]\n",
    "\n",
    "\n",
    "    # Iterating through each translated sequence\n",
    "    for i in tqdm(range(len(translated_list))):\n",
    "        translated_seq_ids = translated_input_ids[i:i+1]  # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Single sequence from translated batch\n",
    "        reference_seq_ids = reference_input_ids[i:i+1]  # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Corresponding reference sequence\n",
    "        source_seq_ids= source_input_ids[i:i+1] # 2D tensor of shape [1, max seq_length in the tokenized list] containing token_ids for Corresponding reference sequence\n",
    "        source_seq_attention_mask=source_attention_mask[i:i+1]\n",
    "\n",
    "\n",
    "        # Find phrases(subsequences) missing from translated_seq to add them as constraints\n",
    "        constraint, position = find_missing_tokens(translated_seq_ids[0], reference_seq_ids[0])#constraint will be a 1D tensor of token_ids, Sending in values of the one row in translated_seq_ids and reference_seq_ids in find_missing_tokens, so these are 1D tensors(imagine like a list of integers)\n",
    "                                                                                    #it could also return None in case of no mismatch\n",
    "        print(f\"The reference sequence Token id:\\n{reference_seq_ids[0]}\")\n",
    "        print(f\"The Hypotheses sequence Token id:\\n{translated_seq_ids[0]}\")\n",
    "        print(f\"constraint Identified in this iteration at position {position}:\\n{constraint}\")\n",
    "        #In case there is no mismatch in translated and reference list just add the sequence from translated_list in the constrained_translated_list, and continue to next sequence\n",
    "        if constraint==None:\n",
    "            constrained_translated_list.append(translated_list[i]) #.append will add the string as a single element to constrained_translated_list, if we use .extend here it will treat the string as an iterable and add each of its elements(characters) to the constrained_translated_list\n",
    "            continue\n",
    "\n",
    "        constraint_info={\"token_ids\":constraint, \"position\":position}\n",
    "        #Add the constraint to the list of constraints for the current sequence\n",
    "        GBS_constraints[i].append(constraint_info)\n",
    "\n",
    "\n",
    "        #Generated translation using Grid_beam_search\n",
    "        constrained_seq_ids = constrained_beam_search(\n",
    "            input_ids=source_seq_ids,#sending input_ids of one sequence\n",
    "            attention_mask=source_seq_attention_mask, #sending attention mask of 1 sequence\n",
    "            constraints=GBS_constraints[i], #Sending constraints for that one sequence\n",
    "        )\n",
    "\n",
    "        print(f\"token_ids of the Constrained_Beam_Search output which will be decoded and stored as our constrained translation:\\n{constrained_seq_ids}\")\n",
    "        constrained_translation = tokenizer.batch_decode(constrained_seq_ids, skip_special_tokens=True)\n",
    "\n",
    "        constrained_translated_list.extend(constrained_translation) #we use .extend here because it will add each element of the iterable constrained_translation to constrained_translated_list, constrained_translation could be a list of strings, but in this case is just a list containing one string\n",
    "\n",
    "    return constrained_translated_list\n",
    "\n",
    "\n",
    "#Getting the translation of input sequence by deriving constraints for each sequence of target_reference_list subsequences that are missing from model_generated_list\n",
    "constrained_translation_1=constrained_translate(model_generated_list)  #list of Constrained Translated Sequnces in Target Language\n",
    "\n",
    "Constraint_Seq_t=tokenizer(\n",
    "                constrained_translation_1,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "                ).to(device)\n",
    "constraint_seq_input_ids=Constraint_Seq_t[\"input_ids\"]\n",
    "print(f\"The token_ids after tokenizing the constrained translation showing the constrained token_ids are still applied:\\n{constraint_seq_input_ids}\")\n",
    "\n",
    "\n",
    "#BLEU score for fist iteration\n",
    "Base_blue_score= sacrebleu.corpus_bleu(model_generated_list, reference_list_BLEU)\n",
    "print(f\"\\nBase Bleu score:{Base_blue_score} \")\n",
    "#Confirm number of elements in hypotheses list is equal to number of sequences in reference list)\n",
    "if len(constrained_translation_1)==len(reference_list_BLEU[0]):\n",
    "    constrained_bleu_score1= sacrebleu.corpus_bleu(constrained_translation_1, reference_list_BLEU)\n",
    "    print(f\"\\n BLEU score for the constrained translation Sequence:{constrained_bleu_score1}\")\n",
    "\n",
    "else:\n",
    "    print(\"Mismatch in number of hypotheses and reference sequences\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GA1env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "028d3abeca024d53ae3e5cdae8a2bbd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33cb2b8562cc401093bf28905f9c7992",
      "max": 37492126,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7be52ff9086e4a23aea03f9ee8c9a5eb",
      "value": 8996000
     }
    },
    "06cae619c2d54d20b7c7a96d7251f213": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "086f908260d84d97bb4d3d8912c0a2b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "09ee4dd3cef2463d8b737cb744cf08dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ad89b826f9f497ead30f7baf8a22645": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e3153271b46b46b18802558d77e35e03",
       "IPY_MODEL_569b65bef3714fe7b509a1a5009b4d23",
       "IPY_MODEL_6fcad94f7ef24cc4928b9947c3fbf6a6"
      ],
      "layout": "IPY_MODEL_53ca98b67d23472c94e69f5c0eced4bc"
     }
    },
    "0b4a8baf204d4ce0b0b37cee816fdbc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b8936b90e514585b72f13c4817d9972": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0be2e316cc7d4c318b7a297ab2b4487d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d7e39bb4bb84260816bb88c50f0c5b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_243c9cff3283467da77d53498098f44f",
      "placeholder": "​",
      "style": "IPY_MODEL_0be2e316cc7d4c318b7a297ab2b4487d",
      "value": "Generating train split: 100%"
     }
    },
    "0d9eaa8a86b14fb1908655bb54ebd653": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_71e509449c0d4c6cb22beb7b4d188f64",
       "IPY_MODEL_86f763b88c1346ddabe2c220ef97378e",
       "IPY_MODEL_b6b69c495c5a48189588bd6c4603ef59"
      ],
      "layout": "IPY_MODEL_8d195114a8924734a0b65aee2eda03fc"
     }
    },
    "0fd29c5d9dce47f099165770aa9aadbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4523290cd5164a888da737020c61816a",
      "max": 293,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3d20b18d095840abadf655019c736fa4",
      "value": 293
     }
    },
    "10867f5fbda745b998402670846c745b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf09a4cfcf5e4f399283f027e0873638",
      "placeholder": "​",
      "style": "IPY_MODEL_b34fdb1c9f3441718fd2a5f020a13304",
      "value": " 2.60M/2.60M [00:00&lt;00:00, 26.8MB/s]"
     }
    },
    "1250a925e7d54075bb77fe786c4a46b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fadd31106d0040f8bbaeb2cea0aa63c3",
      "max": 11142,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_909736211af4453dbd4d8182dcc43369",
      "value": 11142
     }
    },
    "129805d8e3824296a5b7d71d2908f418": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_41dbbe791ff7441ba81ada90bd2c6d92",
       "IPY_MODEL_ac30a9e2da75411cb4fc67833d670eff",
       "IPY_MODEL_3c3fa86c347845eabcabee90f59120fd"
      ],
      "layout": "IPY_MODEL_1858f92c7f054ad79116ecb5651e05b9"
     }
    },
    "14484fa305a549a89eaaa2cbdbfaf5ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "170d50285d494223975f8acf8ae5f859": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "184426f440be45b8a12e70aaf1ee4fe5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1858f92c7f054ad79116ecb5651e05b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "188de32fec5a4d99bf2c273f7f499caf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4b82c2e33f545209328ad9844431036",
       "IPY_MODEL_e73f35d00fa3489c9d8e1c769cd23daa",
       "IPY_MODEL_633b2b2083ca4aeebb414db8d9ccd73c"
      ],
      "layout": "IPY_MODEL_75a767165bde458f9857392bc42b0e06"
     }
    },
    "18b22262fc0743ea8831b233d277c572": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1aaed2a7462842a68ee4c51ae31ca0fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f06d89519d54f4da8f77c8c5918ea5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f1f903e12ce4ee994a975f5fd575740": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20bd7ecbc7214a1cba1070782e3a2730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ab866bb475446cfb3b05418c3f647c3",
      "placeholder": "​",
      "style": "IPY_MODEL_f1ac1ecf6a9045358d732f0d6b1f9b79",
      "value": "target.spm: 100%"
     }
    },
    "243c9cff3283467da77d53498098f44f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2470471eb85745fe8e840cc600909a22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "249ccc99b69248028204cd8d4246553b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2aa35e39c3334c1cad4da515e10a7ff0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b3dcf5dcfe54414ae565e1b949c77ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2cc09841c13b404fbc633dc73362fec3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e8cf6651c734b30bdaf116471104bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7aa822851fd34ff28ac6c04d4fd0f6d9",
      "max": 230432840,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_086f908260d84d97bb4d3d8912c0a2b8",
      "value": 230432840
     }
    },
    "2eabbd91292f4edf8d8c9f0d28a63fed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33cb2b8562cc401093bf28905f9c7992": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35dbdbb9897e4f3caf4e20dfe634b68b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3933e32825584d60a2869d1e43fedc5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_758ac985798a4787a7a71152c9720257",
       "IPY_MODEL_eb8225580b7b4e7fbe0e6ace9cba81c8",
       "IPY_MODEL_e2e2ad09a5344601ac6d9d5b32d120e7"
      ],
      "layout": "IPY_MODEL_6316e90c6b99422a88d91d1dd4b7773f"
     }
    },
    "39b8714087e946deabd07f5febf9b345": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b735b3fcc9143a0a2d71d6a2e080917": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be3460aeba1345f3880666dcd00b5f73",
      "placeholder": "​",
      "style": "IPY_MODEL_14484fa305a549a89eaaa2cbdbfaf5ce",
      "value": "README.md: 100%"
     }
    },
    "3bc2db4f4a6842feb84120eca9cdc203": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c3fa86c347845eabcabee90f59120fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_903c56e5e4624049a5a53d1ab693e004",
      "placeholder": "​",
      "style": "IPY_MODEL_2aa35e39c3334c1cad4da515e10a7ff0",
      "value": " 42.0/42.0 [00:00&lt;00:00, 1.03kB/s]"
     }
    },
    "3d20b18d095840abadf655019c736fa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "41dbbe791ff7441ba81ada90bd2c6d92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4cff771ef77407fa1a18e5548467457",
      "placeholder": "​",
      "style": "IPY_MODEL_0b4a8baf204d4ce0b0b37cee816fdbc6",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "4284fdefcf994b86a9074d3484a17204": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4463c9290f15422a8adafe1cb295504b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da60fabd31f54e32a22a5c8ba8984ae9",
       "IPY_MODEL_028d3abeca024d53ae3e5cdae8a2bbd6",
       "IPY_MODEL_810143c13796416798a7721f85ef630d"
      ],
      "layout": "IPY_MODEL_a374c8b87b134ad7b27fec4acab742de"
     }
    },
    "4523290cd5164a888da737020c61816a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4706c3a0eaad418fab93513aee76c2e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "473a902fc7ff4d5f9c258c1c134e6b7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "476c5808e47447cb802ed84a29ba5713": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4ac18157909843e7988bd16a7e25d841": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c091b74ebbd405bab8d85ee5cd4fe30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "50251a019f154bcf9cdc7febefb2cbda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5226f63a4e624a17864a9d721b54fcda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52d26450f7c7480180e1fe2e25851950": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53ca98b67d23472c94e69f5c0eced4bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54808fb2dbf342468c4366a24fe1cac1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54bebf90a0d94ba5af048c1c48244e4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "556cecdea2c84412a8e364b175c41acf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2cc09841c13b404fbc633dc73362fec3",
      "max": 802781,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bb02a36931bd47b19efac803b3b5d371",
      "value": 802781
     }
    },
    "5616732afb1041ec950140169a1f23a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d54ab52b9e84d3f83f7ddd5162ac1d2",
      "placeholder": "​",
      "style": "IPY_MODEL_39b8714087e946deabd07f5febf9b345",
      "value": " 1516162/1516162 [00:04&lt;00:00, 729573.20 examples/s]"
     }
    },
    "569b65bef3714fe7b509a1a5009b4d23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f06d89519d54f4da8f77c8c5918ea5b",
      "max": 2818,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_170d50285d494223975f8acf8ae5f859",
      "value": 2818
     }
    },
    "5aae6a9322e34c1083a69f5b4215a3f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d83611947a67419a8254c64f980cf740",
      "placeholder": "​",
      "style": "IPY_MODEL_cdb659084c684fd0abbece743e7c9997",
      "value": " 293/293 [00:00&lt;00:00, 8.85kB/s]"
     }
    },
    "5ab866bb475446cfb3b05418c3f647c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b81decca076416b87157df71421c6ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d54ab52b9e84d3f83f7ddd5162ac1d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f52ab484d4745a9908cbd90a9dadd43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_52d26450f7c7480180e1fe2e25851950",
      "max": 1516162,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2470471eb85745fe8e840cc600909a22",
      "value": 1516162
     }
    },
    "6316e90c6b99422a88d91d1dd4b7773f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "633b2b2083ca4aeebb414db8d9ccd73c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88a5219ea8b748188857435e53937ff1",
      "placeholder": "​",
      "style": "IPY_MODEL_35dbdbb9897e4f3caf4e20dfe634b68b",
      "value": " 587k/587k [00:00&lt;00:00, 14.3MB/s]"
     }
    },
    "6407f72cf934474caedcc19f901fc00e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64d49d183a5a47ab9e8fbe7dc8b054c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c357670a18845b2bcf43c50f1dfd217": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4284fdefcf994b86a9074d3484a17204",
      "max": 1080169,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8757719e8a614c70873ca4e97ff61fc2",
      "value": 1080169
     }
    },
    "6c4e66d24ed949609a0b5edee4424e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6c756938690e486fb39b4c02280ed98e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fcad94f7ef24cc4928b9947c3fbf6a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea42373920a04ea2bc0e27fb3326355d",
      "placeholder": "​",
      "style": "IPY_MODEL_06cae619c2d54d20b7c7a96d7251f213",
      "value": " 2818/2818 [00:00&lt;00:00, 74920.60 examples/s]"
     }
    },
    "7115f8d4a830471187da3471582ec9b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "716affa78b7f4b4fadf118189e54601c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71e509449c0d4c6cb22beb7b4d188f64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef7608c62aef432c988ce68ef9ec6ef9",
      "placeholder": "​",
      "style": "IPY_MODEL_f053944ad1eb4145b82e7e9c7fd793a0",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "7402994cb51b463fae900c5c77bd01d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_91ddefbcaf74464395e062620158528f",
       "IPY_MODEL_cd22b855862c4622bc98c25c12b75ce2",
       "IPY_MODEL_10867f5fbda745b998402670846c745b"
      ],
      "layout": "IPY_MODEL_85371c7a17274c10b007488f428cdf1f"
     }
    },
    "7533d927e6d946db9f0514f158556457": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecea6e3b42e24ce3a206069db4f667b5",
      "max": 2998,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9c27544f090a4326933304d933ffcef4",
      "value": 2998
     }
    },
    "758ac985798a4787a7a71152c9720257": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54808fb2dbf342468c4366a24fe1cac1",
      "placeholder": "​",
      "style": "IPY_MODEL_be9ce2b517a14583b92b4ada34332dd4",
      "value": "config.json: 100%"
     }
    },
    "75a767165bde458f9857392bc42b0e06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79bb9344bc424697b18e36429b6eec4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7aa822851fd34ff28ac6c04d4fd0f6d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7be52ff9086e4a23aea03f9ee8c9a5eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7c14fae85ab7474f877603dfe05838db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ce6a60ab6014cbeaa82a7bd48923664": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ebbcb301aff41e7bd9f273c4347f085": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "810143c13796416798a7721f85ef630d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d11a0d13b20c40ada5b4e027a5f89b1b",
      "placeholder": "​",
      "style": "IPY_MODEL_dbf2a33450264864908701bbe9c3f7e3",
      "value": " 8996000/37492126 [52:29&lt;3:18:35, 2391.58 examples/s]"
     }
    },
    "82a6ab736045435a98b5d199a6cc635e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b07f233f49034cc8b44af809a820f821",
       "IPY_MODEL_7533d927e6d946db9f0514f158556457",
       "IPY_MODEL_d41a83d52c5c4b2c8e596c10dbe9fd51"
      ],
      "layout": "IPY_MODEL_7c14fae85ab7474f877603dfe05838db"
     }
    },
    "84ead4ae903c460cb6808095d27b0de6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed9e8b12d97442589bce6db4e6f5ce67",
      "placeholder": "​",
      "style": "IPY_MODEL_3bc2db4f4a6842feb84120eca9cdc203",
      "value": " 11.1k/11.1k [00:00&lt;00:00, 487kB/s]"
     }
    },
    "85371c7a17274c10b007488f428cdf1f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "869999a5c12448a08e5b145d1c9dc235": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "86f763b88c1346ddabe2c220ef97378e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1c876415334463a8d939e13b0651d7f",
      "max": 306991893,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c4e66d24ed949609a0b5edee4424e95",
      "value": 306991893
     }
    },
    "8757719e8a614c70873ca4e97ff61fc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "88a5219ea8b748188857435e53937ff1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c21bc1c2f654fb5ab643358996c3f2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d195114a8924734a0b65aee2eda03fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dc72a507e914a4ea6e255ee91da63ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "903c56e5e4624049a5a53d1ab693e004": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "909736211af4453dbd4d8182dcc43369": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "90c5cb554daf49eea0744ce1fe2a8b33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91ddefbcaf74464395e062620158528f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c95399dcaa774f57b27febab43cecc44",
      "placeholder": "​",
      "style": "IPY_MODEL_716affa78b7f4b4fadf118189e54601c",
      "value": "vocab.json: 100%"
     }
    },
    "93a053b30eb74061b467c239710ea422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da0c64dba28a428a9b345f21ec81883c",
       "IPY_MODEL_556cecdea2c84412a8e364b175c41acf",
       "IPY_MODEL_97f0875d41e545b8b500ce6ba7b81471"
      ],
      "layout": "IPY_MODEL_d8da40f672314e669b6c83c6ec6d4ce4"
     }
    },
    "97f0875d41e545b8b500ce6ba7b81471": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90c5cb554daf49eea0744ce1fe2a8b33",
      "placeholder": "​",
      "style": "IPY_MODEL_4706c3a0eaad418fab93513aee76c2e5",
      "value": " 803k/803k [00:00&lt;00:00, 12.0MB/s]"
     }
    },
    "9b00f47f4fe64296b2fd0ca71cca76b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c27544f090a4326933304d933ffcef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9c7e8ce8482c414b90067a2e4804b848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d98be13cfc1449e8234f62c3b9b7e19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09ee4dd3cef2463d8b737cb744cf08dd",
      "placeholder": "​",
      "style": "IPY_MODEL_869999a5c12448a08e5b145d1c9dc235",
      "value": " 538k/538k [00:00&lt;00:00, 7.82MB/s]"
     }
    },
    "9e79a9b62a4448f89699a2882d23b6da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5226f63a4e624a17864a9d721b54fcda",
      "max": 538000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4c091b74ebbd405bab8d85ee5cd4fe30",
      "value": 538000
     }
    },
    "9ff1f2f3ba1246bc969014ed0f700e2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6407f72cf934474caedcc19f901fc00e",
      "placeholder": "​",
      "style": "IPY_MODEL_9c7e8ce8482c414b90067a2e4804b848",
      "value": " 230M/230M [00:06&lt;00:00, 30.7MB/s]"
     }
    },
    "a374c8b87b134ad7b27fec4acab742de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa926e098e0d41278247cd3635b0363d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac30a9e2da75411cb4fc67833d670eff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d06807057e444c75903bb2025340e8c4",
      "max": 42,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79bb9344bc424697b18e36429b6eec4a",
      "value": 42
     }
    },
    "adbb547788ca487490713d32bc4250e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3b735b3fcc9143a0a2d71d6a2e080917",
       "IPY_MODEL_1250a925e7d54075bb77fe786c4a46b5",
       "IPY_MODEL_84ead4ae903c460cb6808095d27b0de6"
      ],
      "layout": "IPY_MODEL_7115f8d4a830471187da3471582ec9b3"
     }
    },
    "b07f233f49034cc8b44af809a820f821": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b00f47f4fe64296b2fd0ca71cca76b7",
      "placeholder": "​",
      "style": "IPY_MODEL_c0d0d6f8a901488ba897f87477fef023",
      "value": "Generating test split: 100%"
     }
    },
    "b14dfb3ac719409bb9cc33737a5b7029": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dcfaf22176aa451eb7f777f5d2649cd5",
       "IPY_MODEL_2e8cf6651c734b30bdaf116471104bf4",
       "IPY_MODEL_9ff1f2f3ba1246bc969014ed0f700e2f"
      ],
      "layout": "IPY_MODEL_cdfbeca7348649fcace275726dd0b4f0"
     }
    },
    "b34fdb1c9f3441718fd2a5f020a13304": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4d81ed8a52545aa87afc931caffd30e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ac18157909843e7988bd16a7e25d841",
      "placeholder": "​",
      "style": "IPY_MODEL_ce386c557f884dd8926d8c5e498ed7a2",
      "value": "generation_config.json: 100%"
     }
    },
    "b6b69c495c5a48189588bd6c4603ef59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da465b6209e7426690675a84c0ca6a6f",
      "placeholder": "​",
      "style": "IPY_MODEL_6c756938690e486fb39b4c02280ed98e",
      "value": " 307M/307M [00:02&lt;00:00, 151MB/s]"
     }
    },
    "b7d075e89ab941299a0776c528a40591": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba8d2018a1c1448f8c60896089a5a48f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb02a36931bd47b19efac803b3b5d371": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "be3460aeba1345f3880666dcd00b5f73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be9ce2b517a14583b92b4ada34332dd4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bebe60db38544e2ab7fc92b63cb800fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1aaed2a7462842a68ee4c51ae31ca0fd",
      "placeholder": "​",
      "style": "IPY_MODEL_8dc72a507e914a4ea6e255ee91da63ee",
      "value": " 1.08M/1.08M [00:00&lt;00:00, 8.29MB/s]"
     }
    },
    "bf09a4cfcf5e4f399283f027e0873638": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0d0d6f8a901488ba897f87477fef023": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1c876415334463a8d939e13b0651d7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3a50cec758f4029a1533960ef613b55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4d81ed8a52545aa87afc931caffd30e",
       "IPY_MODEL_0fd29c5d9dce47f099165770aa9aadbb",
       "IPY_MODEL_5aae6a9322e34c1083a69f5b4215a3f0"
      ],
      "layout": "IPY_MODEL_184426f440be45b8a12e70aaf1ee4fe5"
     }
    },
    "c95399dcaa774f57b27febab43cecc44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9a5ef7c8a9940af8110c3fec3834870": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_20bd7ecbc7214a1cba1070782e3a2730",
       "IPY_MODEL_6c357670a18845b2bcf43c50f1dfd217",
       "IPY_MODEL_bebe60db38544e2ab7fc92b63cb800fb"
      ],
      "layout": "IPY_MODEL_2eabbd91292f4edf8d8c9f0d28a63fed"
     }
    },
    "cc2a4cf1aa40493abfee8dd75bd3c5a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0d7e39bb4bb84260816bb88c50f0c5b2",
       "IPY_MODEL_5f52ab484d4745a9908cbd90a9dadd43",
       "IPY_MODEL_5616732afb1041ec950140169a1f23a3"
      ],
      "layout": "IPY_MODEL_d175477ce2914c509b199e7aa7303539"
     }
    },
    "ccb4bc48223849bfa9b3b7d175430889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd22b855862c4622bc98c25c12b75ce2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7d075e89ab941299a0776c528a40591",
      "max": 2601758,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b3dcf5dcfe54414ae565e1b949c77ae",
      "value": 2601758
     }
    },
    "cdb659084c684fd0abbece743e7c9997": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cdbbf4cb571a4c88a71fd75bdb6f1879": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cdfbeca7348649fcace275726dd0b4f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce386c557f884dd8926d8c5e498ed7a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d01c2b6c6c8445b8bd60d924f5eb3b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ce6a60ab6014cbeaa82a7bd48923664",
      "placeholder": "​",
      "style": "IPY_MODEL_50251a019f154bcf9cdc7febefb2cbda",
      "value": "validation-00000-of-00001.parquet: 100%"
     }
    },
    "d06807057e444c75903bb2025340e8c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d11a0d13b20c40ada5b4e027a5f89b1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d175477ce2914c509b199e7aa7303539": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d41a83d52c5c4b2c8e596c10dbe9fd51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa03bebf1d5740e3ba2e66cedebd642b",
      "placeholder": "​",
      "style": "IPY_MODEL_476c5808e47447cb802ed84a29ba5713",
      "value": " 2998/2998 [00:00&lt;00:00, 54032.61 examples/s]"
     }
    },
    "d4b82c2e33f545209328ad9844431036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b81decca076416b87157df71421c6ba",
      "placeholder": "​",
      "style": "IPY_MODEL_cdbbf4cb571a4c88a71fd75bdb6f1879",
      "value": "test-00000-of-00001.parquet: 100%"
     }
    },
    "d83611947a67419a8254c64f980cf740": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8da40f672314e669b6c83c6ec6d4ce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da0c64dba28a428a9b345f21ec81883c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f1f903e12ce4ee994a975f5fd575740",
      "placeholder": "​",
      "style": "IPY_MODEL_aa926e098e0d41278247cd3635b0363d",
      "value": "source.spm: 100%"
     }
    },
    "da465b6209e7426690675a84c0ca6a6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da60fabd31f54e32a22a5c8ba8984ae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ebbcb301aff41e7bd9f273c4347f085",
      "placeholder": "​",
      "style": "IPY_MODEL_fe57e399e0714707aa48c5b36b2dbc87",
      "value": "Map:  24%"
     }
    },
    "dbf2a33450264864908701bbe9c3f7e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dcfaf22176aa451eb7f777f5d2649cd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe89d2cb1bf64b05b6bf8cb9790bd329",
      "placeholder": "​",
      "style": "IPY_MODEL_18b22262fc0743ea8831b233d277c572",
      "value": "train-00000-of-00001.parquet: 100%"
     }
    },
    "e20f5a4e42aa4854bbbe3d8e48b329f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d01c2b6c6c8445b8bd60d924f5eb3b07",
       "IPY_MODEL_9e79a9b62a4448f89699a2882d23b6da",
       "IPY_MODEL_9d98be13cfc1449e8234f62c3b9b7e19"
      ],
      "layout": "IPY_MODEL_0b8936b90e514585b72f13c4817d9972"
     }
    },
    "e2e2ad09a5344601ac6d9d5b32d120e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54bebf90a0d94ba5af048c1c48244e4f",
      "placeholder": "​",
      "style": "IPY_MODEL_e6dd792e3a2a4ef9b8b612687d756820",
      "value": " 1.38k/1.38k [00:00&lt;00:00, 26.1kB/s]"
     }
    },
    "e3153271b46b46b18802558d77e35e03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccb4bc48223849bfa9b3b7d175430889",
      "placeholder": "​",
      "style": "IPY_MODEL_473a902fc7ff4d5f9c258c1c134e6b7c",
      "value": "Generating validation split: 100%"
     }
    },
    "e6dd792e3a2a4ef9b8b612687d756820": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e73f35d00fa3489c9d8e1c769cd23daa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c21bc1c2f654fb5ab643358996c3f2e",
      "max": 586531,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_64d49d183a5a47ab9e8fbe7dc8b054c7",
      "value": 586531
     }
    },
    "ea42373920a04ea2bc0e27fb3326355d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb8225580b7b4e7fbe0e6ace9cba81c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba8d2018a1c1448f8c60896089a5a48f",
      "max": 1381,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_249ccc99b69248028204cd8d4246553b",
      "value": 1381
     }
    },
    "ecea6e3b42e24ce3a206069db4f667b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed9e8b12d97442589bce6db4e6f5ce67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef7608c62aef432c988ce68ef9ec6ef9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f053944ad1eb4145b82e7e9c7fd793a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1ac1ecf6a9045358d732f0d6b1f9b79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4cff771ef77407fa1a18e5548467457": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa03bebf1d5740e3ba2e66cedebd642b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fadd31106d0040f8bbaeb2cea0aa63c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe57e399e0714707aa48c5b36b2dbc87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe89d2cb1bf64b05b6bf8cb9790bd329": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
